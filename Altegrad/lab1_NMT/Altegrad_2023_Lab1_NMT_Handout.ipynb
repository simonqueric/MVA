{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCJvlnvsKALE"
      },
      "source": [
        "<center><h2>ALTeGraD 2023<br>Lab Session 1: NMT</h2><h3> Neural Machine Translation</h3> 10 / 10 / 2023<br> Dr. G. Shang and H. Abdine</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB6pvLvlKbtD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "from nltk import word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualization = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqIFlSfYTwk8"
      },
      "source": [
        "## Define the Encoder / Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc8cQTFkKmif"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    to be passed the entire source sequence at once\n",
        "    we use padding_idx in nn.Embedding so that the padding vector does not take gradient (always zero)\n",
        "    https://pytorch.org/docs/stable/nn.html#gru\n",
        "    '''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # fill the gaps # (transform input into embeddings and pass embeddings to RNN)\n",
        "        # you should return a tensor of shape (seq, batch, feat)\n",
        "        embeddings = self.embedding(input)\n",
        "        hs, _ = self.rnn(embeddings)\n",
        "        return hs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn9iO9wNT2p7"
      },
      "source": [
        "## Define the Attention layer / Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwUAUDL4KmoM"
      },
      "outputs": [],
      "source": [
        "class seq2seqAtt(nn.Module):\n",
        "    '''\n",
        "    concat global attention a la Luong et al. 2015 (subsection 3.1)\n",
        "    https://arxiv.org/pdf/1508.04025.pdf\n",
        "    '''\n",
        "    def __init__(self, hidden_dim, hidden_dim_s, hidden_dim_t):\n",
        "        super(seq2seqAtt, self).__init__()\n",
        "        self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim_t, hidden_dim)\n",
        "        self.ff_score = nn.Linear(hidden_dim, 1, bias=False) # just a dot product here\n",
        "\n",
        "    def forward(self, target_h, source_hs):\n",
        "        target_h_rep = target_h.repeat(source_hs.size(0), 1, 1) # (1, batch, feat) -> (seq, batch, feat)\n",
        "        # fill the gaps #\n",
        "        # implement the score computation part of the concat formulation (see section 3.1. of Luong 2015)\n",
        "        concat_output = self.ff_concat(torch.cat((target_h_rep, source_hs), dim=2)) # (seq, batch, feat)\n",
        "        scores = self.ff_score(torch.tanh(concat_output)) # should be of shape (seq, batch, 1)\n",
        "        scores = scores.squeeze(dim=2) # (seq, batch, 1) -> (seq, batch). dim = 2 because we don't want to squeeze the batch dim if batch size = 1\n",
        "        norm_scores = torch.softmax(scores, 0)\n",
        "        source_hs_p = source_hs.permute((2, 0, 1)) # (seq, batch, feat) -> (feat, seq, batch)\n",
        "        weighted_source_hs = (norm_scores * source_hs_p) # (seq, batch) * (feat, seq, batch) (* checks from right to left that the dimensions match)\n",
        "        ct = torch.sum(weighted_source_hs.permute((1, 2, 0)), 0, keepdim=True) # (feat, seq, batch) -> (seq, batch, feat) -> (1, batch, feat); keepdim otherwise sum squeezes\n",
        "        if visualization :\n",
        "            return ct, norm_scores\n",
        "        else : \n",
        "            return ct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNnGEa5cT9ka"
      },
      "source": [
        "## Define the Decoder layer / Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7tLaq4PK90q"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''to be used one timestep at a time\n",
        "       see https://pytorch.org/docs/stable/nn.html#gru'''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "        self.ff_concat = nn.Linear(2*hidden_dim, hidden_dim)\n",
        "        self.predict = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input, source_context, h):\n",
        "        # fill the gaps #\n",
        "        # transform input into embeddings, pass embeddings to RNN, concatenate with source_context and apply tanh, and make the prediction\n",
        "        # prediction should be of shape (1, batch, vocab), h and tilde_h of shape (1, batch, feat)\n",
        "        embeddings = self.embedding(input)\n",
        "        _, h = self.rnn(embeddings, h)\n",
        "        concat_output = torch.cat((source_context, h), dim=2) \n",
        "        h_tilde = torch.tanh(self.ff_concat(concat_output))\n",
        "        prediction = self.predict(h_tilde)\n",
        "\n",
        "        return prediction, h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUT6D3JETX8H"
      },
      "source": [
        "# Define the full seq2seq model / Task 4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYX0K3dNK-c9"
      },
      "outputs": [],
      "source": [
        "class seq2seqModel(nn.Module):\n",
        "    '''the full seq2seq model'''\n",
        "    ARGS = ['vocab_s','source_language','vocab_t_inv','embedding_dim_s','embedding_dim_t',\n",
        "     'hidden_dim_s','hidden_dim_t','hidden_dim_att','do_att','padding_token',\n",
        "     'oov_token','sos_token','eos_token','max_size']\n",
        "    def __init__(self, vocab_s, source_language, vocab_t_inv, embedding_dim_s, embedding_dim_t,\n",
        "                 hidden_dim_s, hidden_dim_t, hidden_dim_att, do_att, padding_token,\n",
        "                 oov_token, sos_token, eos_token, max_size):\n",
        "        super(seq2seqModel, self).__init__()\n",
        "        self.vocab_s = vocab_s\n",
        "        self.source_language = source_language\n",
        "        self.vocab_t_inv = vocab_t_inv\n",
        "        self.embedding_dim_s = embedding_dim_s\n",
        "        self.embedding_dim_t = embedding_dim_t\n",
        "        self.hidden_dim_s = hidden_dim_s\n",
        "        self.hidden_dim_t = hidden_dim_t\n",
        "        self.hidden_dim_att = hidden_dim_att\n",
        "        self.do_att = do_att # should attention be used?\n",
        "        self.padding_token = padding_token\n",
        "        self.oov_token = oov_token\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.max_size = max_size\n",
        "\n",
        "        self.max_source_idx = max(list(vocab_s.values()))\n",
        "        print('max source index',self.max_source_idx)\n",
        "        print('source vocab size',len(vocab_s))\n",
        "\n",
        "        self.max_target_idx = max([int(elt) for elt in list(vocab_t_inv.keys())])\n",
        "        print('max target index',self.max_target_idx)\n",
        "        print('target vocab size',len(vocab_t_inv))\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.encoder = Encoder(self.max_source_idx+1, self.embedding_dim_s, self.hidden_dim_s, self.padding_token).to(self.device)\n",
        "        self.decoder = Decoder(self.max_target_idx+1, self.embedding_dim_t, self.hidden_dim_t, self.padding_token).to(self.device)\n",
        "\n",
        "        if self.do_att:\n",
        "            self.att_mech = seq2seqAtt(self.hidden_dim_att, self.hidden_dim_s, self.hidden_dim_t).to(self.device)\n",
        "\n",
        "    def my_pad(self, my_list):\n",
        "        '''my_list is a list of tuples of the form [(tensor_s_1, tensor_t_1), ..., (tensor_s_batch, tensor_t_batch)]\n",
        "        the <eos> token is appended to each sequence before padding\n",
        "        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence'''\n",
        "        batch_source = pad_sequence([torch.cat((elt[0], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
        "        batch_target = pad_sequence([torch.cat((elt[1], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
        "        return batch_source, batch_target\n",
        "\n",
        "    def forward(self, input, max_size, is_prod):\n",
        "        if is_prod:\n",
        "            input = input.unsqueeze(1) # (seq) -> (seq, 1) 1D input <=> we receive just one sentence as input (predict/production mode)\n",
        "        current_batch_size = input.size(1)\n",
        "        # fill the gap #\n",
        "        # use the encoder\n",
        "        source_hs = self.encoder(input) # We capture the global context of the sentence \n",
        "        # = = = decoder part (one timestep at a time)  = = =\n",
        "        target_h = torch.zeros(size=(1, current_batch_size, self.hidden_dim_t)).to(self.device) # init (1, batch, feat)\n",
        "\n",
        "        # fill the gap #\n",
        "        # (initialize target_input with the proper token)\n",
        "        target_input = torch.LongTensor([self.sos_token]).repeat(current_batch_size).unsqueeze(0).to(self.device) # init (1, batch)\n",
        "        pos = 0\n",
        "        eos_counter = 0\n",
        "        logits = []\n",
        "        scores = []\n",
        "        while True:\n",
        "            if self.do_att:\n",
        "                if visualization :\n",
        "                    source_context, score = self.att_mech(target_h, source_hs) # (1, batch, feat)\n",
        "                else : \n",
        "                    source_context = self.att_mech(target_h, source_hs)\n",
        "            else:\n",
        "                source_context = source_hs[-1, :, :].unsqueeze(0) # (1, batch, feat) last hidden state of encoder\n",
        "            # fill the gap #\n",
        "            # use the decoder\n",
        "            #print(target_input.shape)\n",
        "\n",
        "            prediction, target_h = self.decoder(target_input, source_context, target_h)\n",
        "            #print(\"PREDICTION SHAPE\", prediction.shape)\n",
        "            logits.append(prediction) # (1, batch, vocab)\n",
        "            # fill the gap #\n",
        "            # get the next input to pass the decoder\n",
        "            #print(prediction.argmax(dim=2))\n",
        "            target_input = prediction.argmax(dim=2)\n",
        "            #print(target_input.shape)\n",
        "            eos_counter += torch.sum(target_input==self.eos_token).item()\n",
        "            if visualization :\n",
        "                scores.append(score)\n",
        "            pos += 1\n",
        "            if pos >= max_size or (eos_counter == current_batch_size and is_prod):\n",
        "                break\n",
        "        to_return = torch.cat(logits, 0) # logits is a list of tensors -> (seq, batch, vocab)\n",
        "\n",
        "        if is_prod:\n",
        "            to_return = to_return.squeeze(dim=1) # (seq, vocab)\n",
        "        if visualization :\n",
        "            return (to_return, scores)\n",
        "        else : \n",
        "            return to_return\n",
        "\n",
        "    def fit(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience):\n",
        "        parameters = [p for p in self.parameters() if p.requires_grad]\n",
        "        optimizer = optim.Adam(parameters, lr=lr)\n",
        "        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.padding_token) # the softmax is inside the loss!\n",
        "        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "        # we pass a collate function to perform padding on the fly, within each batch\n",
        "        # this is better than truncation/padding at the dataset level\n",
        "        train_loader = data.DataLoader(trainingDataset, batch_size=batch_size,\n",
        "                                       shuffle=True, collate_fn=self.my_pad) # returns (batch, seq)\n",
        "        test_loader = data.DataLoader(testDataset, batch_size=512,\n",
        "                                      collate_fn=self.my_pad)\n",
        "        tdqm_dict_keys = ['loss', 'test loss']\n",
        "        tdqm_dict = dict(zip(tdqm_dict_keys, [0.0, 0.0]))\n",
        "        patience_counter = 1\n",
        "        patience_loss = 99999\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            with tqdm(total=len(train_loader), unit_scale=True, postfix={'loss':0.0, 'test loss':0.0},\n",
        "                      desc=\"Epoch : %i/%i\" % (epoch, n_epochs-1), ncols=100) as pbar:\n",
        "                for loader_idx, loader in enumerate([train_loader, test_loader]):\n",
        "                    total_loss = 0\n",
        "                    # set model mode (https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "                    if loader_idx == 0:\n",
        "                        self.train()\n",
        "                    else:\n",
        "                        self.eval()\n",
        "                    for i, (batch_source, batch_target) in enumerate(loader):\n",
        "                        batch_source = batch_source.transpose(1, 0).to(self.device) # RNN needs (seq, batch, feat) but loader returns (batch, seq)\n",
        "                        batch_target = batch_target.transpose(1, 0).to(self.device) # (seq, batch)\n",
        "\n",
        "                        # are we using the model in production\n",
        "                        is_prod = len(batch_source.shape)==1 # if False, 2D input (seq, batch), i.e., train or test\n",
        "                        if is_prod:\n",
        "                            max_size = self.max_size\n",
        "                            self.eval()\n",
        "                        else:\n",
        "                            max_size = batch_target.size(0) # no need to continue generating after we've exceeded the length of the longest ground truth sequence\n",
        "\n",
        "                        unnormalized_logits = self.forward(batch_source, max_size, is_prod)\n",
        "                        sentence_loss = criterion(unnormalized_logits.flatten(end_dim=1), batch_target.flatten())\n",
        "                        total_loss += sentence_loss.item()\n",
        "                        tdqm_dict[tdqm_dict_keys[loader_idx]] = total_loss/(i+1)\n",
        "                        pbar.set_postfix(tdqm_dict)\n",
        "                        if loader_idx == 0:\n",
        "                            optimizer.zero_grad() # flush gradient attributes\n",
        "                            sentence_loss.backward() # compute gradients\n",
        "                            optimizer.step() # update\n",
        "                            pbar.update(1)\n",
        "\n",
        "            if total_loss > patience_loss:\n",
        "                patience_counter += 1\n",
        "            else:\n",
        "                patience_loss = total_loss\n",
        "                patience_counter = 1 # reset\n",
        "\n",
        "            if patience_counter > patience:\n",
        "                break\n",
        "\n",
        "    def sourceNl_to_ints(self, source_nl):\n",
        "        '''converts natural language source sentence into source integers'''\n",
        "        source_nl_clean = source_nl.lower().replace(\"'\",' ').replace('-',' ')\n",
        "        source_nl_clean_tok = word_tokenize(source_nl_clean, self.source_language)\n",
        "        source_ints = [int(self.vocab_s[elt]) if elt in self.vocab_s else \\\n",
        "                       self.oov_token for elt in source_nl_clean_tok]\n",
        "\n",
        "        source_ints = torch.LongTensor(source_ints).to(self.device)\n",
        "        return source_ints\n",
        "\n",
        "    def targetInts_to_nl(self, target_ints):\n",
        "        '''converts integer target sentence into target natural language'''\n",
        "        return ['<PAD>' if elt==self.padding_token else '<OOV>' if elt==self.oov_token \\\n",
        "                else '<EOS>' if elt==self.eos_token else '<SOS>' if elt==self.sos_token\\\n",
        "                else self.vocab_t_inv[elt] for elt in target_ints]\n",
        "\n",
        "    def predict(self, source_nl):\n",
        "        source_ints = self.sourceNl_to_ints(source_nl)\n",
        "        if visualization :\n",
        "            logits, scores = self.forward(source_ints, self.max_size, True) # (seq) -> (<=max_size, vocab)\n",
        "        else : \n",
        "            logits = self.forward(source_ints, self.max_size, True)\n",
        "        target_ints = logits.argmax(-1).squeeze() # (<=max_size, 1) -> (<=max_size)\n",
        "        target_nl = self.targetInts_to_nl(target_ints.tolist())\n",
        "        if visualization :\n",
        "            return scores, ' '.join(target_nl)\n",
        "        else : \n",
        "            return ' '.join(target_nl)\n",
        "\n",
        "    def save(self, path_to_file):\n",
        "        attrs = {attr:getattr(self,attr) for attr in self.ARGS}\n",
        "        attrs['state_dict'] = self.state_dict()\n",
        "        torch.save(attrs, path_to_file)\n",
        "\n",
        "    @classmethod # a class method does not see the inside of the class (a static method does not take self as first argument)\n",
        "    def load(cls, path_to_file):\n",
        "        attrs = torch.load(path_to_file, map_location=lambda storage, loc: storage) # allows loading on CPU a model trained on GPU, see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/6\n",
        "        state_dict = attrs.pop('state_dict')\n",
        "        new = cls(**attrs) # * list and ** names (dict) see args and kwargs\n",
        "        new.load_state_dict(state_dict)\n",
        "        return new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5RprtnBK-ia"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from torch.utils import data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgkVw6lVUIT3"
      },
      "source": [
        "## Prepare the Data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "datl5SFtJ9Br",
        "outputId": "13c3c990-83dc-44fa-fc91-0972fd052348"
      },
      "outputs": [],
      "source": [
        "#!wget -c \"https://onedrive.live.com/download?resid=AE69638675180117%2199291&authkey=!AMIEuRcvDQWgoZo\" -O \"data.zip\"\n",
        "#!wget -c \"https://onedrive.live.com/download?resid=AE69638675180117%2199292&authkey=!ANLtZTfpmk6tcE0\" -O \"pretrained_moodle.pt\"\n",
        "#!unzip data.zip\n",
        "\n",
        "path_to_data = './'\n",
        "path_to_save_models = './'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZCiFl61LPQj"
      },
      "outputs": [],
      "source": [
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.pairs) # total nb of observations\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        source, target = self.pairs[idx] # one observation\n",
        "        return torch.LongTensor(source), torch.LongTensor(target)\n",
        "\n",
        "def load_pairs(train_or_test):\n",
        "    with open(path_to_data + 'pairs_' + train_or_test + '_ints.txt', 'r', encoding='utf-8') as file:\n",
        "        pairs_tmp = file.read().splitlines()\n",
        "    pairs_tmp = [elt.split('\\t') for elt in pairs_tmp]\n",
        "    pairs_tmp = [[[int(eltt) for eltt in elt[0].split()],[int(eltt) for eltt in \\\n",
        "                  elt[1].split()]] for elt in pairs_tmp]\n",
        "    return pairs_tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCsAk4ILTkEc"
      },
      "source": [
        "## Training / Task 5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "kSZ-cvSuLQVt",
        "outputId": "2f31f98d-4836-4a8c-aade-2ce53f4b3e4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data loaded\n",
            "data prepared\n",
            "= = = attention-based model?: True = = =\n",
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch : 0/4:   0%|                      | 3.00/2.13k [00:01<16:20, 2.17it/s, loss=8.93, test loss=0]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_12123/3892438329.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m                          max_size=30) # max size of generated sentence in prediction mode\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_save_models\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'my_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_12123/3255609593.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience)\u001b[0m\n\u001b[1;32m    142\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mloader_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flush gradient attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                             \u001b[0msentence_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                             \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "do_att = True # should always be set to True\n",
        "is_prod = False # production mode or not\n",
        "\n",
        "if not is_prod:\n",
        "\n",
        "    pairs_train = load_pairs('train')\n",
        "    pairs_test = load_pairs('test')\n",
        "\n",
        "    with open(path_to_data + 'vocab_source.json','r') as file:\n",
        "        vocab_source = json.load(file) # word -> index\n",
        "\n",
        "    with open(path_to_data + 'vocab_target.json','r') as file:\n",
        "        vocab_target = json.load(file) # word -> index\n",
        "\n",
        "    vocab_target_inv = {v:k for k,v in vocab_target.items()} # index -> word\n",
        "\n",
        "    print('data loaded')\n",
        "\n",
        "    training_set = Dataset(pairs_train)\n",
        "    test_set = Dataset(pairs_test)\n",
        "\n",
        "    print('data prepared')\n",
        "\n",
        "    print('= = = attention-based model?:',str(do_att),'= = =')\n",
        "\n",
        "    model = seq2seqModel(vocab_s=vocab_source,\n",
        "                         source_language='english',\n",
        "                         vocab_t_inv=vocab_target_inv,\n",
        "                         embedding_dim_s=40,\n",
        "                         embedding_dim_t=40,\n",
        "                         hidden_dim_s=30,\n",
        "                         hidden_dim_t=30,\n",
        "                         hidden_dim_att=20,\n",
        "                         do_att=do_att,\n",
        "                         padding_token=0,\n",
        "                         oov_token=1,\n",
        "                         sos_token=2,\n",
        "                         eos_token=3,\n",
        "                         max_size=30) # max size of generated sentence in prediction mode\n",
        "\n",
        "    model.fit(training_set,test_set,lr=0.001,batch_size=64,n_epochs=5,patience=2)\n",
        "    model.save(path_to_save_models + 'my_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf0rN4RPToom"
      },
      "source": [
        "## Testing / Task 6:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCvZmwWoCTUT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/simon/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhXbQjP_YrgY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n",
            "= = = = = \n",
            " I am a student. -> je suis étudiant . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I have a red car. -> j ai une voiture rouge . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I love playing video games. -> j adore jouer à jeux jeux jeux vidéo . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " This river is full of fish. -> cette rivière est pleine de poisson . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " The fridge is full of food. -> le frigo est plein de nourriture . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " The cat fel asleep on the mat. -> le chat <OOV> endormi sur le tapis . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " my brother likes pizza. -> mon frère aime la pizza . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I did not mean to hurt you -> je n ai pas voulu intention de blesser blesser blesser blesser blesser blesser . blesser . blesser . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " She is so mean -> elle est tellement méchant méchant . <EOS>\n",
            "= = = = = \n",
            " Help me pick out a tie to go with this suit! -> aidez moi à chercher une cravate pour aller avec ceci ! ! ! ! ! ! ! ! ! ! ! ! ! ! <EOS>\n",
            "= = = = = \n",
            " I can't help but smoking weed -> je ne peux pas empêcher de de fumer fumer fumer fumer fumer fumer fumer fumer fumer fumer urgence urgence urgence urgence urgence urgence . urgence urgence . urgence urgence .\n",
            "= = = = = \n",
            " The kids were playing hide and seek -> les enfants jouent cache cache cache cache caché caché caché caché caché caché caché caché caché caché caché caché caché caché caché dentifrice perdre caché risques rapide caché risques éveillés\n",
            "= = = = = \n",
            " The cat fell asleep in front of the fireplace -> le chat s est en du du pression peigne peigne cheminée portail portail portail portail portail portail portail portail indépendant oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux\n"
          ]
        }
      ],
      "source": [
        "is_prod = True # production mode or not\n",
        "\n",
        "if is_prod:\n",
        "    model = seq2seqModel.load('pretrained_moodle.pt')\n",
        "\n",
        "    to_test = ['I am a student.',\n",
        "               'I have a red car.',  # inversion captured\n",
        "               'I love playing video games.',\n",
        "                'This river is full of fish.', # plein vs pleine (accord)\n",
        "                'The fridge is full of food.',\n",
        "               'The cat fel asleep on the mat.',\n",
        "               'my brother likes pizza.', # pizza is translated to 'la pizza'\n",
        "               'I did not mean to hurt you', # translation of mean in context\n",
        "               'She is so mean',\n",
        "               'Help me pick out a tie to go with this suit!', # right translation\n",
        "               \"I can't help but smoking weed\", # this one and below: hallucination\n",
        "               'The kids were playing hide and seek',\n",
        "               'The cat fell asleep in front of the fireplace']\n",
        "\n",
        "    for elt in to_test:\n",
        "        print('= = = = = \\n','%s -> %s' % (elt, model.predict(elt)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code for Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x, y = model.predict('I saw a black dog')\n",
        "test = [list(x[i].detach().numpy()) for i in range(6)]\n",
        "test = np.array(test)\n",
        "n, m, _ = np.shape(test)\n",
        "test = test.reshape(n, m)\n",
        "y\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "plt.imshow(test.T, cmap=\"gray\")\n",
        "ax.xaxis.tick_top()\n",
        "ax.xaxis.set_ticks(list(range(6)), labels = [\"j\", \"ai\", \"vu\", \"un\", \"chien\", \"noir\"])\n",
        "ax.yaxis.set_ticks(list(range(5)), labels = [\"I\", \"saw\", \"a\", \"black\", \"dog\"])\n",
        "plt.rcParams.update({'font.size': 30})\n",
        "plt.savefig(\"heat_map_2.jpeg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArsAAAKBCAYAAABXgmNJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA+Y0lEQVR4nO3dd5hkVZ3/8fd3ZkB2QJAkRsQcV8TFwCoCRlQQIwiuC4YfoiIgiMqKgFlXXTMKorLLiiKYIyYQTAgIuKCASlSCIEFB0sx8f3+cU8ylpntid9+q0+/X89ynus69VX3u7eqqT5177jmRmUiSJEktmtN3BSRJkqTpYtiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpKkCUXEpDkhImIm6yKtrHl9V0CSJI2eiJiTmYvqzzsBjwE2An4A/Cgz/9zdRhpVkZl910GSJI2oiDgIOGSo+CfAqzLzfAOvRp3dGCRJ0oQiYhfgTcDngWcCmwHHAlsBx0XEQzJz0dK6O0h9sxuDJEkCSj/crKd8a4C9P3A68K7MPLdutlNEXAG8Djg2Il6UmefawqtRZdiVJEkAdILuG4G7A9sCxw2CbkSslpm3Zebe9fo0A69GnmFXkiTdLiI2AF4CPAy4Bri8ls/NzNvq7cKhwPuFiHhpZp7dW8WlSdjHRpIk3S4zr6aE3e8BGwI7R8S9MnNhXb8wIubWn/cGPgRsChwWEas7JJlGjS27kiRp2DnAAZSc8Azg1RHxkcz8CywOvLWFd7+IuBX438y8tcc6SxNy6DFJkmahZfWvrReoPQz4GPB44H3AJzPzys42cwctvtKoMuxKkjTLDE0YsTWlG8IDgOuBrwAXZ+Zfa+B9KPAJ4HFMEHillTGTFzMadiVJmkWGgu5bgDcCd+5schXwLcpwYxdMEHjfDXw6M6+Y2ZqrFUND3L0PuC0zD5yu3+cFapIkzSKdoLsv8A7g68DTgQcDbwAuBl4GfCwi7lu3/y3wWuBnwNuA3QYXqUkrqhN0XwvsD2wTEfeYrt9ny64kSQ2b6HRxRDwK+BrwR8q0v3+o5fOA+wCHAk8DPg78R2beUNc/EngXsH9nkglpuQydVVgT+BJwHfD2zDxvun6vLbuSJDUoIjaC0pI7wXBgdwfuCXy9E3TnZOaCzPwjpYX3bGAHYJ3BgzLzN8ALDLpaGZ2g+0JgO2Ab4JuDoDtdw9YZdiVJakxtgb0gIj4Ei08bd9wfmAssqNuvNtT6+3vgh8C9KS28twcRhxfTqoiIJ1BadA8ELgN+WcvnTfA6nRKGXUmS2nML8E/AQ+rpYuD2bgoAg1PGzwGoM6PNqdvMzcybKRepQR2Tf7qCiGadPwIHARsB9wNeDJCZC2zZlSRJy1TD6nmUvre7ZeaNtTWNzFxQNzsVOBd4ekQcNOhLGRF36oybuw2QlFZeaZV0zgxcAXwG+ADwd2CXiNiqrsvpCLyGXUmSGlJnN5uTmZdm5pV11IWTI+KgzjbXAbsDVwODURnIzFsAIuLZwPOBMykjMUgrZHCmYKB7ZiAzLwf+lzJu8wOBt0TEYwbbTXXgdTQGSZLG2NCYpUH5bF/UWf9I4GRgPvC2zHxnLb8TpRvDx4C7Ulp7TwE2BLYGVgO2ysxzZm5v1IKhURc2pXRXWB+4FDgpM2+q6+4B7Aa8Ffgp8JbM/FVdF1PVdcawK0nSmFraLFQRsR3wm8y8JCIeTBkjdz3g4Mx8R91mHvBwylBjD6IEksuBs4DXT+dwUGrTUNB9E+XMwYadTX4IHJ6Zx9Vt7gq8khJ4T6YE3lOntE6GXUmSxltEfBH4eWZ+tN5/L/CqunwtM2+dLPDW7dcCNqBMGfw74IbMvH6Gd0MNiYg3U2bb+wZwBHAFsBVwMHAD8KbMPKpuOwi8b6b0Jd8jM389VXWZt+xNJEkaPUtr1ZxNImILYEfgORFxKfAQyhTAnwZ+UYPu3Mw8r16o9jPgbRFBJ/DeWCeOuKiHXVBjImJrSovuV4G3ZuZva/k9gNsoFz7+YLB9Zv4lIj5D6WrzCkpf8qmrjy27kqRxVvuknl+Hy5qVIuJ5lCvc16I0ZH0A+HhmXtLZZm69eK3bwntQpw+vXx40JSJiH8rFZztk5vdqX/IdatnawBaZeVHtRrNGZ4a+DYFFmfnXqayPozFIksZWRLycMmLAS+oFV7PK4Ir3zPwqZXD+OcBC4OpB0I2IuXWbhZ1hyZ4AXAMcFBHvqesNulolnVEUtgKuZ3Hr7fOA9wJ3oQbdWr4JsGftRkNmXjXVQRcMu5Kk8XY9ZVrbD1LG65xVgXcwFXBEbEIZV/fnwM2Ubgovq9ss7AwDtWgo8M4D9oiIDXqovtozCLuXUaaZ3jIinkbpu7su8LhO0AV4G6XLzfrTWSnDriRpbGXml4EDgAsoQ2jNxsCbwJ8op4mfCuwM3Ap8shN4Fw2mY63hd34NvA+lBJAp7SOp9nXH0Y2I+XCHswO/oAxddzDwSUqL7h2CbkTsBmxJmTr4L9NaV/vsSpJGXWeGrzuMKdv5eTvg7ZThs14HfKHVPryT9a0d9MmtP7+IcgX8nYBXZ+bnOts9CXg88PnM/PMMVVsNGRpebAdgW+D4zPxaLVsTOBJ4AWXq6qdn5smdxz8fOIQSiJ+VmRdOZ30djUGSNJIGYbbeDsLd+sDVw+E3M79Vuwu+HfgokBFxzGDw+lYMhYztKbNP3Qv4H+B84B8AmXlsRCTlorVD68gLn4uIZwLvoZxi/t8+9kHjbeg1+EbgDcCNwOmDbeoU1UdQui48Cdg1Iu5DmXp6F+CFwOrA1tMddMGWXUnSiKlXZN+UmTcMtd4eBLwaeFpmnj1Ja++OwCco3fT2AY5rJfAO7eeBlP6OSdnX6yndOD7XDQ8R8ULgMEroOJVyQdA84MmZedaM7oCaEhEHAO8CjqaM/PHLWt4Nw08B/h9laLyBv1Fm6ttrpiYtMexKkkZGRDwc+B7wEeBTNfAOQu1hlA/Os4GdM/OcSQLvEcDLgb9S+gx+JjNv6WePpl4dgeJTwHeAzwEbA8+lXAF/GPBfmfnHzvbbUsY8vRtlYP/XOTOaVkU9q/A/wFeA92bm7zvr5gPzu/3A62vwbpS+uycBF2bmtTNWX8OuJGlU1AkSvlzvvhs4cjAGZ13/fmA/yixfOw4F3jtl5i0RsTNlJqY1gTWAh4/zbGDDfXQj4guUMXL3zsxz63BP9wPeCexECcIfHAq8G1AuWotxPhYaDfX/8DXAUzotugHsDryYMrHJVyjTAvd+BsE+u5KkUXIKpT/fEZRWWSLi9sCbmfvXcWP3Ab4UEYPAu3qn9fYZlBmY3kiZbGKsw13nlPC+wMXAPYH/qUF3EIT/WPtPJrBH3b4beK9xHF1Nhfr/9wBgYSfoPpMy3e/zgEsoF6W9mvJl8xU9VfV2hl1J0sioLbS/oHRX+DQTB95968VXrweOi4gXZebZdbvnAo8D/jszj+9jH6ZDRGxKubBsLnADpY/uHSaCyMxLI+JN9e4ewMKI+Fhmnm/Q1cpYyqx6vwN2iIhTgeuAxwKLgL2A4ynj7X4B2K22Ap836GbUB8OuJGmk1BEYfs7SA+9+NfDuC/wiIg6nXIT1VMpwRl+e8MnHVGaeFRF7UVrLHgk8MiK+mpkLhrYbBN6FwGuBWyLigMy8beZrrXE2dKHZYykjeJyVmX+ps+7dG9iG0g/328CBmXlB5/HXA78FLu4z6IJ9diVJI2AwRmx3uLG66l8pgXdDyugDw3149wf+HXh4LfotsFNmnjOD1Z9WQ+Pn7k6ZRGMDykV635rkMfcBDqRcrPa7GausmjAUdPcC9qd0TXgpcFpm3lYnb7kXZdix67rjWtdxnj8BfAPYs+8xrw27kqSRERFPy8wfdO4Hyw6896SMN/t34JLMvGpmaz11lnLauLvNKykD8t8ZeFFmfn+S7eYNt/xKyzI0sslbKV+avk35v/tGLb/9C9gEj38x8CZKi+9TuxdK9sWwK0k9WZ5gM5tExL9TZl06NDP37JQvM/C2YKg1bWvKVL4PAC4Hvjw0fu4rKcdhLcqoFM30T9ZoiIhXAIdS/ic/lJnnTrJdUCaI2Jhy4ej2ddUzR+UMi2FXkmZQRNwD2Cwzv913XUZNRGxOOfX5GOBjmbl3Z92kgbfbEjWuhoLuWyizUq0DLKBcX3MNZXSJ72fmn+p2r6DMGLcW8MJui7i0sur/2rrAcZTuMi/OzN921m8HPJry+jw2M39ZuzR8iRJ0jwP+IzP/MOOVn8ScvisgSbNFRDwKOAb4Zg006sjM0yjjdP4CeF1EfKSzLoHBRWtXAf8BvDoi1hz3oAt3GF5sP+AdlNPGTwPmA7tSJsg4Atg+Ilarj/kMcBBwLXB8RDy5h6qrMfX/6c7A5sA5g6AbEY+KiEMp/XAPoYyG8tOI2KoO+/fvlGH/dh+loAu27ErSjKhXM3+Hckr6SODDk/V5a90EkyQM39+Ucvp0CyZu4d2CMmD9DcBjZnImpulUvwx9EzgDeOPgtHFE7EBp8V4LeERm/mmoJfi1lJEXnufMaJoKEXE34EzKRCTvpnSn2QG4O+Xsyncow429AzgKeO0odysy7ErSNIuIBwPfpbRIvt0uDEVEPHQwUsAyAu9/ZeYbOuvmULo6XD0KF79MlYh4AXAspUvCV2rZ84F3UU4rPy4zL66njOlMokFErDPuk2do5k3UBagzIspLKf+DawI3AecAewK/ycyb6xfPG4EvZubLZ7ruK8JuDJI0jeoHwkspfd8+2g26EXGfiNg9Ig6JiCdGxF36qudMq2PB/izK1L6DySRu/0yqU4zuQ5kJbd+I+EBn3aLMPGWcg25naLWuB9bb0+o2z6O0qq0LPD4zL67rNwS+ExEbDh5o0NWKql8wB6MurB4Rd42I9QdlmXkUsDWwN6VV92mZ+avOMGKvokxyMni9TvSaHgmGXUmaXnMorZPXZubnASJi44jYDTgd+BSl3+WxwMsiYs4of2hMocsoQxMdHBE7wYSB91RK30AogfdzM13JqdDdp3p/jU7IuE9n1aCl9om168J7WRx0L+ps9wbgCZSr36UVNtQN5hXA14Fzgf+LiK/VL99rZebpmfmxzPxh9wtVfX3uXh/zDbi9r+9IMuxK0vSaA/wJuHdEvLiG3E8Bn6W0iOxFOTW4gHIh0pxR/tCYKrXVaCfgvsA7hwNvRAxm+PwLcClwHrBrRGzUS4VXQSdUvKaeIr653j8EOCIiHlA3/TpwJeXLzwcpV7s/rht0a0v4DpQvR+fP1D6oHfU1OHhNHgwcBtyPMuvgaZRRTz4DvDYi1h567Px6VuaDlAkldhmMDjLKDLuSNI2yTNP6LeDPwNGUkLs55RT9jpn58cw8FLgA2ITS2tmMQSt1t7V68HNmHgv8G6WF8l11MPpBN4XBZAhPAH4FPAe4X2ZeOYPVnzIR8T7g48Dh9f6BlFD7W8pkGFC6bHyZcjzuA7xgKOjuSBmFIoFDMnPwOK2gpb0uW9c5q7Ar8BbgvynvRf+P8v/4MUqXmu2AQSiOiLg/8CPgPZQzM1uOyji6y+IFapKW0MK4paMmIv6VMqXt9cDvMvP/OuseRRmb8jRK/94FLRz/oVOlawNrAKtRLizrXlz1IuB/KV8I3pmZn63lzwHeB5yQma+Z6fpPpdp6exTwOOAsYFPgQ8CnMvP3ne3uC3yEEjROBk4AfkMZv/TplDF3nzwuIWMUDb0uN6RcgDUPuGpwqr6198Du/tRQvxrlTML9KUH3zNrd5tnA+ylnFbbIzIsGxysi7ko5q7A6ZZKTK3rZmZVg2JW0hIi4S2Ze13c9xk0NKlsCm1FGXvh+HTt2aY95JLAf8Hxg18FV+ONuKFDsBuwIPJISKn4OfC8zD+9s/0JKy/c8Sh/A24CtKK2YW2bm2J6yjzptb0SsDlxEuVjxD5QZpi6uXTYWQml1q/149wReQGntB7iOEnwPGOdj0beh1+WrKWPD/jPlTPevKZMkfKSuH+vAGxH3A+40GPFkaN3dKa/FT2fmnhExl/KFatBP/PbuMxHxcODCzPxH3S5zzGZ+NOxKuoOI+GfKm/4eWQat13KIMo7u0ZS+bwkEJcDsBRw90dXyUWYieg1lIPb9MvPDM1bhaTTUinQIcCBwBeWU/T2Ah9VNP5yZ+3Ye90TKqf4HU47dOcDLsjN70ziLiGdQhqC7mhJ4P52Zr6rr5mbmwsGxi4hBK/jjKK+ls4G/j/JYpuMkIg6iXPz4a+AkytBauwPrA0dl5q791W7VRcS9KBePnUgZs3kwMcTg9bUR5VqCz2bmqyJie0qL7h2Cbn3M74EfjPXZlcx0GZOF+uVkWWUuLquyAC+i9NP6IXAnX2PLdcw2o8xidQbwulq2JaVf2wLgzcDane3vRhm/cgFwIfCqzro5fe/PFB6Xl1JaaD8GPKyWrUs5Rf+3+jr7z6HHbAA8gtLatm7f+zDFx+O+lNbaLSmt24soYWOwfm5Lf/9RWIC5E5TtSAm3nwEe2infs/5Nbhn31x5wb8qFsDcBXwQe3lkXlAlKLqF0kdmbcrHjFcB9h55nP8pU1XuM82eBLbtjotNnZl1gI8pVkH8C/pjlApixP+Wi0RERWwHnZuaVEfGQrDM5aUkRcW/KFMDzgQMz81u1/O2UFs0rgbvWnz+ZtXtIlFmv7g18MzN/VsvuMLHCuKp9AucDn6dc2f3kzDy7rhu8l21BaXVaDdgpM49t6T1ssr9lRNwpM2+JiI0pIyo8BvhcZr5iaLtNKcH3jJmpcVsi4h6ZeVn9+fZWc8oXis8B2wLPysxTa1/V51JmA1sH+NfMvCQiVs/MW3vahVVW35v2oYTZ44B3ZKevd0TsS+kTfwtlcojHZ+aFnfU7UMZ5/iulX+/Y9NFdQt9p22XZC/WbPqX16CeUFqRFlGn8jgae0XcdXdpYGGpVorxRLqKEkd7rN2oLpYXklZRWym7r7LvrcfsopXXzPMpp+f2A9Trb/VP3ufrenyl+7axPabX+2fA+dm5fXI/TR1o4BhMdC8pFiZsCm02w3b2BU1iyhfdpwC8pUyLP73t/xm2hnBW4Dfjg8N+EEmb/APxwUE656OpcSsvmJp3HPAzYtu/9WcVjsTFlmLCFLNnC+wjgq5Swe1zddg3KF4LXdo7JQ/rej1VdBuMYakQNxsOLiEcDP6b09fos5UX4MMpplydGxCFZr2CWVlYu2RJ1HeXL1RciYlGWoaJUZWZGxI3AyZl5GNzeWvJm4Ajg/Zl5aW3Fewdl2te1IuKjmXltZt7Ufa4edmFKDI3bec/M/DPlQrMA7l77B/5lsH09boOZl/4ObB0RgylJx/Y4wBLHYj/KF5y7AKtHxKGU/qCnAtTXxgspLby71eN0KuU0+ybA/8vMf8z8Xoy91ShfQF8fEf/IzLfWz9GghNugNBZBCbrvYYK+qpT+41dExEnj+nfI0kL9kXp3H4CIeGdmnp2ZZ0fEhynH43mUC0LPpxyLjSlnj5+SLZzZ6zttuyy5sGQLyT0oHwq/A7brlG9CeWNcRBmPsfe6u7SxcMeWqZ0op+IXAS/qu26jtlCGLbpz/fnRlCucf8Qd+wJuTRlH9xf1OD6m73pP07F4G6V/3z3r/aPq/r60s00Mvb4uBb7Td92n4Vi8se77WZSxdX9F6aN9AqVbR3fbewHfpASwmygXoz28j3qP+8LiswaPprTgLqKcvh+sXxP4Wi0/oH6uXkkZw7n7PK+hnL5//fBn8igvTHJ2hNJf/L9Y3ML7z511D6RMaHMKpQ/vj4A3Affue3+m7Lj0XQGXzh8DNp+kfFtKf5q3dMo2Bb5Q/2F375R7ymviY7h+/dJw577rMorLst7MgV0MvLcfi3mUC/fWnGDdjvUY7VjvDz54X00ZimxHYIe+92Eaj83Rdf93rvf/rd5fRD0dTL1giNLCtgvldPN7KKdOx64bA4tPj8/tlG0MnEk5C/egTtkbKKeMT6a0mHWfZz3gKZSL9+7R936N8wLMq7eP7QTe93TW78HiC9GuAtYZevxzagj+FfWL2ygvnddg90vkOvUz716dsrsBH54o8A6O27I+C8Z16b0CLvUPAd+jnM57Sqds8EE56P83uJp5086Hyh6d7Vej9IG7e9/7M0oLZein0ymD+Z8E7N93nUZpGXqD3BzYmXLqdStgtc66l8z2wEvpC3gY5Ur6U+rPWwBr1fWDq7mfM/SYEyiDsE943Md96bxXPay+Rr7TWXcQiwPvq6gtlpTxTU+jnCrdpO99WIl97va9Hv4fekR9vxkOtGtSLhaaMPC6TMnfZfjM6DMpow7cBryrU/6J+pr8PWX85zXrZ+h+lCHyrmLE+6oC63d+7r5Xv6a+R11HaZ3+CuVL1BqULgrdwPuIzuO6X9jG7ovnUo9V3xVwqX8IeGt9UT5xgnV7UU5/PYHyzWzQorvH0HavpZwGm7CFeDYulKk1FwEX18AxGO7osL7rNgrL0If0/p0wO1g+Azyhs82sDbyUq+avrR+aF3ReSxdRxutcu/6PLqK0Cj29fsB8uYab3frehxk4RnehTAixCPj3Tvmb64frIspp+ivr/UsYal0ah4XS4HAesNdQ+UvqPv4E+GWnvBsi5rM48P6UoS4NLqv0d+m+n+1F+UJxOqUhafCe1u3S8PFO+UWULjgLKKfyR7obCeWC9fPpnNmt5W/pfOZ9vb5X3Vb/595BGXJsPcrsfYPA+7C+92faj1ffFXDp/DHgPvX2ocDTO+U71xfvj1nc1+jVQ499HOXq3Z/QOW0xm5f6gXRZDWyDVvFHM8H4lrNxofPNvfMG+W3KEDz/zOIreL8MbNPZtht4/63v/ZihY7Vh/QD8GfD8WvYgSr+2P9bg8rZafiB3/MJwE7DPRMd9HBeWbDmbO3R/y/rhesTQa+xZlAv0flnfxw4ZvOeN20LpWrYI+B9gjU75NvV9+gZK17PHTvL4QeC9Afg/YKu+96mlhdIX9zbKkIDb19fevrVsEWVK6sG2u9Tg973693wZY9CNhNJivai+/+xayx5V35sPZfEZlI0p3ajOq+9TB1Gm+71HfY+/CfgOI96KvcrHq+8KuAz9QcqH6k2Ub2PP7JR/q/PheUAtG5w6fATlQpBrgRf3vQ+jsFA63D+N0l/rUbVs0K/poZQJE2Z94K3H48WUUT6O4I7D0pxaPxwW1g+CrTvrdqa0gNwI3HncA9wyjs86lItBzwV2GVo3v77OzqeMNvC0Wv48SmvmPkPHraWuC8P9/ebUZT6lRWkBsMUEj1u9hWNB6Q96t/rzIzvlW1FmSVtEmUxj3iSPn19fI1cyht04RnWhdCv6Ww1wDxha93jgzwwF3rpuicknRn2hhPgbKGdIXkwJ9r/vBN1BRlijvk9dQGnBfkwt34TSFetqxqBv8iodq74r4DLBH6V8A11AmcZwu1r2WMopr0WUb6GPpnRpeAFlYPZFwOs7z9Fs+FiO47d/fbM7Hjihls2lXAU++Od/CAZeKBOUfJfSYvnYWjaP0pfyGsqpwA9TAu/XuWOf8hdRL75pdaGcKjyP0kJ0OYtHXeieLl2D0oVoIfCVpTzXWIe7oX15c/3f+TKl68YGQ+tfVdcfWUPd4ItmdH/uez+m6FjsD/wDeEWnbMvO+8s7lvLY+Yz5TF2jtlBGj1kEvKTenzP0unsCi1t4u314V6u3Y/W6BJ5NCbwXAj+gdp8Z7Hdnu/ks7tb3qU75vYC79r0f036c+q6AyyR/mNLBfBFl+tFBa9GTOsH2Fsq3sdsofXNe3XlsMx+qK3nsnl+PzyLgV8PHZZLAe2zf9e7pWG1AaVkaTHE7px6T6yjzxK9OOV1/cT1OXxm8HmfDwuJThddQThcOWvKGT+Xfpx6jKwbbtLpQvjjuRunWcRuldf+nlFamu3W2+zHl4rPBMRurELECx2PwfnMB8LJO+RMpQzgt0YroMi1/h8H7+r71mO89wTaDz4CP1m1uoE5oMs4L5dqAG+o+/aYT3Iffpx5M+WJ2ErNs5KbeK+CylD/O4sB7JvDUTvlelL42XwJeAfxLZ92sDrqd47Ati2ea23f4+HTeGB9MacVcxCwdxYLSujs4HofUN82D6AzTRuljORjC56t0Zv5qfamB97q67//ZfS1xx5aT71G6MjTfSlL3d536f/YVFnex+hnlavY7s3ic2U/2XdcZOBbd08mTBd5JW3hdpvRv8cR6vD8P3KmWDd7fBu//r6N8Mb2u/t027LveU7Dfz+x85h3SKZ/T2e95lC+gJ9AZvWE2LIMXgEZURLyGcsXoWcCBmfntpWzbzLzyUyEitqV8IbiB0s/5v2v5nKyz6WRmRsQDgVsy85I+6zsKIuLHwD0pc6Rf2yk/m9It5ALgx5n5u56q2IuIeAbltTSH8uXp07V88BrajHJx3/nA9pn59/5qO/MiYjvgyZQv4nMoLUfnUVo9r6BMenN+fzWcfhHxbEp3l2uAgzPzc7X8iZTJNrYBPpyZ+/ZXy/ZFxFqUa1yeRDnjedgE23yC8oXsPcBtmfmHma3l9Oi8Ty0A3jx4n+qs35nyJeDjlG6PC2e+lj3pO227LHthcQvvr4Fnd8pn1TezlTx2g/5Ml1CvWK3lTfUbnKJjtQHlg/pHQ+U7U07Rb993HXs+Ps+iDGF0DfB2FrcaPZoyecAiZsHwYkPHJIbub04ZF/wiFveLvAXYqO+6ztDx6L7fdFt4n0AZAutGGmhFHPWFctH2rfX1txd3HI/2mZQZ6po841Bfg3+vr8MPUPrkrkcZZeI0Sre1B/Rdz5lebNkdE50W3jMpM6l9t5bbmrsMQy0ub82hFt5eKzdC6rzxxwAvpPTX/S7lNPVewD9RRhW4rL8a9q++lo6mtAr9lvKBujZl7Mr3ZuaH63az9v8yIuZQujgcQBkV5cDMPKffWs2cpbTwPh64PDMv7rN+s0VE/AtlKM75lO41Z1D+b59BOZ3/xGz0bENEPIvyPrU25YvnnSmTZNxEGf961vw/Dhh2x0gn8P4aeFNm/qjnKo2NzgfQlZR+l0uc2hJExIMpQ/bcl3IqbB5wKfCs2fgGOZF6qvCLlED3fcp8838eHJ/Z/iWq202I0rf7H33XaaYNvd98IDM/2XOVZqWIeCjlOoRnU0Lv3ykTvrw8M3/bY9WmXX2fOobyPn48pQ/9jZl5XZ/16othd8xExOuAj1AuGHpZZl7fb43GR/22+y3gHMq3eo/dBCJiE8qYjZtRTvf9b2Ze2GulRkxEPBM4ltJackBmfrGWr5aZt/VauREwm1u2B3y/GQ0RsRpwP8qIKVcCl2bmNf3WambU96lvU97Ht8jMG3uuUm8Mu2Ok02LyIcrsO1tk5il912ucRMTTgUsy89y+66LxVi/K+iJ2j9EkfL9R32oL78Wz/TU4r+8KaPl1PkBvAW6mXPWsFZCZ3++7DmpDZn4rInainCo8KCLWyMzDDLoa8P1GfcvM4/uuwygwLI2Zeop5a8qEEk0MlyKNqyxDAe5I6eO8Z0Ss03OVJElDbNkdP3+l9AP7SmZe1XdlpNkuM79Tx3S+xH6ZkjR67LM7hiJibs6mwaAlSZJWkmFXkiRJzbLPriRJkppl2B1TEbF733UYRx63FecxWzket5XjcVtxHrOV43FbOeN43Ay742vsXmwjwuO24jxmK8fjtnI8bivOY7ZyPG4rZ+yOm2FXkiRJzfICtaVYc801c9111+27GhO68cYbWXPNNfuuxoQuu+yyvqswdvw/lCRplV2dmRsOFzrO7lKsu+667L333n1XY+wcfPDBfVdh7Nx00019V0GSpHF38USFdmOQJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1KxZF3YjIiMi+66HJEmSpt+sC7uSJEmaPQy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7A7JCJ2j4jTIuK0G2+8se/qSJIkaRUYdodk5uGZuXlmbr7mmmv2XR1JkiStAsOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNWte3xWYaZkZfddBkiRJM8OWXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWpWZGbfdRhZc+fOzfnz5/ddjbGz9tpr912FsbPVVlv1XYWxdMYZZ/RdhbG0xhpr9F2FsXTmmWf2XQVJS3d6Zm4+XGjLriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmjU3YjYiLIuKizv3dIiIjYrcVeI4j62M2mYYqSpIkacSMTdiVJEmSVtS8viuwCr4K/BK4vO+KSJIkaTSNbdjNzOuB6/uuhyRJkkbXSHVjiGLPiDgnIm6OiD9HxMcjYp0Jtp20z25EPDUiTo6IGyPimoj4WkQ8ZEZ2QpIkSSNj1Fp2PwzsRemacDhwG7AD8DhgdeDWZT1BRLwQOKZue0x9ricCvwB+Mx2VliRJ0mgambAbEf9KCbp/BB6bmdfU8rcAJwB3By5exnOsBRwGLAK2zMzTOus+BOwzLZWXJEnSSBqlbgwvq7fvGgRdgMy8GThgOZ9jB2A94Ohu0K0OYTn6+EbE7hFxWkSclpnL+WslSZI0ikYp7D663v5kgnUnAwtW5TnqBW1nLusJMvPwzNw8MzePiOX4lZIkSRpVoxR2BxehXTm8IjMXAn9dleeorliJekmSJGlMjVLYHXQx2Gh4RUTMBdZfleeo7rYS9ZIkSdKYGqWw++t6u9UE67Zk+S6mm/Q56vBlj1qpmkmSJGksjVLYPbLeviUi1hsURsQawHuW8zm+DlwL7BIRmw+tO4TF3RwkSZI0C4zM0GOZ+bOI+BjwOuDsiDiOxePsXstyTAucmTdExO6U8XVPjojuOLuPAE4CnjRNuyBJkqQRM0otuwB7U8Lu9cCrgJ2B44GnshwTSgBk5nHAtsDpwI7AHsA1wBbAhVNfZUmSJI2qcCzZyc2dOzfnz5/fdzXGztprr913FcbOVltN1FVdy3LGGWf0XYWxtMYaa/RdhbF05pln9l0FSUt3emYOd2MduZZdSZIkacoYdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJalZkZt91GFkbb7xxvuENb+i7GmNnr7326rsKY+eYY47puwpjabXVVuu7CmNpjz326LsKY+mqq67quwqSlu70zNx8uNCWXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNWtWhN2I2CQiMiKO7LsukiRJmjmzIuxKkiRpdjLsSpIkqVkjEXa73Qzqz1+MiKsj4uaIOC0itpvgMXeKiDdHxG8i4h8R8beIODkidhza7hDgwnp31/p7Bstu0793kiRJ6su8visw5D7Ar4ALgKOA9YCdgK9HxFMz8wSAiFgdOB7YCjgX+AQwH3ghcExEPCoz/6M+54nAXYC9gbOAr3V+35nTujeSJEnq1aiF3a2BQzLzbYOCiDga+B6wP3BCLd6PEnS/CzwnMxfUbd9GCcsHRMS3MvPnmXliRFxECbtnZuYhS6tAROwO7A6w7rrrTt2eSZIkacaNRDeGjouBd3YLMvN44BLgsZ3ilwMJ7DsIunXbvwDvqHdfuTIVyMzDM3PzzNx8rbXWWpmnkCRJ0ogYtbB7ZmYunKD8UmBdgIi4M/AA4LLMPHeCbX9cbzebnipKkiRpXIxa2L1ukvIFLK7rOvX28km2HZTfZWqqJEmSpHE1amF3eVxfb+82yfq7D20nSZKkWWrswm5m/h34I3DPiHjgBJtsU29/3SkbdI2YO511kyRJ0mgZu7BbfRYI4P0RcXuAjYgNgLd2thm4lnJB28YzVkNJkiT1btSGHlteHwCeCewAnBUR36GMs/si4K7Af2bmTwcbZ+YNEXEKsGVEfB44n9La+43M/M2M116SJEkzYizDbmbeGhFPA/YFdgFeR7mI7Sxgn8z8wgQPeynwIWBbYGdKy/CfAMOuJElSo0Yi7GbmRZTwOdn6rScouxl4d12W53f8Adh+5WooSZKkcTSufXYlSZKkZTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLUrMjMvuswsiIi582b13c1xo7HbMVtsskmfVdhLD3wgQ/suwpj6eUvf3nfVRhLZ599dt9VGDuHH35431UYS5dffnnfVRhLCxYsOD0zNx8ut2VXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNWu5wm5EbBIRGRFHRsT9I+K4iPhrRPw9Ir4fEY+o220YEYdHxOURcXNEnBoR20zwfPMi4jUR8cuI+FtE/CMizoiIPSNiiTpFxG4R8eWIuCAibqqP+VlE/Nsk9T2x1ndeRPxHRPw+Im6JiEsj4n0RsfqKHihJkiSNn3kruP0mwCnA74Aj6/3nASdGxBbA94C/AccA6wEvBr4bEQ/KzEsAImI14JvAM4DzgKOBm4FtgI8BjwNeOvR7Pwn8FjgJuBxYH3gWcFREPDgz3zpJfY8GtgS+W+v1LOCNwF2Bl63gvkuSJGnMrGjY3Qo4MDPfNSiIiLcCb6eE4C8Br8nMRXXdD4D/AV5fF4C3UILux4F9MnNh3XYucDjw8og4LjO/3vm9j8jMP3YrUltnvwu8OSI+lZl/nqC+9wcenpnX1Me8BTgL+PeIOCAzr1jB/ZckSdIYWdE+uxcB7x0q++96eydg/0HQrY4GFgCPAqhdFPYErgBePwi6APXn/YAEXtL9BcNBt5bdCnyCEtifMkl93zQIuvUxNwKfp+z35hM9ICJ2j4jTIuK0SZ5TkiRJY2JFW3bP7AbU6rJ6e35m/r27IjMXRsSVwL1q0YMoXRB+DxwYERP9jpuAh3YLImJj4E2UULsx8E9Dj7nnJPWdKLBeWm/XnegBmXk4pYWZiMhJnleSJEljYEXD7vXDBZm5oIbWJdZVC4DV6s/r19sHAgcv5fesNfghIu4H/IoSTk8Gvl9/10JKn+FdKa3KS8jM6yapD8Dcpfx+SZIkNWBFw+6qGgTir2bm85fzMftSQvLLMvPI7oqI2JkSdiVJkqQlzPQ4u+cC1wGPr6MyLI8H1NsvT7Buq6molCRJkto0o2E3MxdQhhe7O/DRiBjue0tE3D0iHtYpuqjebj203TOAV05PTSVJktSCme7GAPAOYFNgD2D7iPgx8GfK2LcPBJ5AGZ7st3X7Qylj4h4bEV+u2z4C2JYy1NlOM1p7SZIkjY0ZD7uZeVtEPBf4N2A3YDvKBWlXARcCb6UMDzbY/jd1FrZ3UiaFmEcZK/f5lC4Rhl1JkiRNaLnCbmZeBEw4Tlhdv7R1m0xQlsBRdVme3/9z4MmTrF7id2fm1kt5riMps79JkiSpcTN9gZokSZI0Ywy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZkVm9l2HkRURGRF9V0OzgK+zlbPGGmv0XYWx9JCHPKTvKoyl5zznOX1XYeyccsopfVdhLC1atKjvKoyl448//vTM3Hy43JZdSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZYxt2I2KTiMiIOLLvukiSJGk0jW3YlSRJkpbFsCtJkqRmjWXYjYhDgAvr3V1rd4bBslvdZk5E7BERp0bEDRFxY/351RExlvstSZKkFTOv7wqspBOBuwB7A2cBX+usO7PeHgXsAlwKHAEk8DzgUOCJwEtmoqKSJEnqz1iG3cw8MSIuooTdMzPzkO76iNiZEnTPAJ6UmTfU8gOBnwC7RMS3M/PoGa24JEmSZlSrp/NfXm/fPAi6AJl5I/CmeveVEz0wInaPiNMi4rRprqMkSZKm2Vi27C6HRwOLKN0dhv0EWAhsNtEDM/Nw4HCAiMhpqp8kSZJmQKstu+sA12TmrcMrMnMBcHXdRpIkSQ1rNexeD6wXEasNr4iIecAGwN9mvFaSJEmaUeMcdhfW27kTrDuDsm9PmmDdk+pjfj1N9ZIkSdKIGOewey1lOLGNJ1j32Xr7noiYPyisP7+33v3M9FZPkiRJfRvbC9Qy84aIOAXYMiI+D5xPae39RmYeHRE7ADsC50TE1yjB+LnAfYEvZebn+6m5JEmSZsrYht3qpcCHgG2BnYEA/gT8pt7/CWUYslfV7X8HfBD45IzXVJIkSTNurMNuZv4B2H6SdYsos6UdOqOVkiRJ0sgY5z67kiRJ0lIZdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJata8visw6jKz7ypoFvB1tnL+8Y9/9F2FsXTGGWf0XYWxdM455/RdhbGz2Wab9V2FsbT66qv3XYWm2LIrSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDVr2sNuRGwSERkRR07375IkSZK6ZkXLbkRsXQP3IX3XRZIkSTNnVoRdSZIkzU6GXUmSJDVrRsNu7b/7xYi4OiJujojTImK7oW0OqV0Otp7k8Uv0/42II2v5/SLidRHxm4i4KSJOrNueUDc9uG6Xk/0OSZIktWPeDP6u+wC/Ai4AjgLWA3YCvh4RT83ME5b24OX0EWBL4NvAd4CFwKl13a7AT4ATO9tfNAW/U5IkSSNqJsPu1sAhmfm2QUFEHA18D9ifxa2vq+LRwGaZeWG3MCKuo4TdEzPzkKU9QUTsDuw+BXWRJElSz2Yy7F4MvLNbkJnHR8QlwGOn6Hf853DQXVGZeThwOEBE5JTUSpIkSb2YyT67Z2bmwgnKLwXWnaLf8aspeh5JkiQ1YCbD7nWTlC+YwnpcMUXPI0mSpAaM4tBji+rtRF0s7rKMx9rtQJIkSbcbxbB7bb299wTrNl/J5xx0n5i7ko+XJEnSGBrFsDvod/uyiLi9dTci7g0ctJLP+dd6u/GqVEySJEnjZSZHY1gumXlKRJwEPAn4VUT8GNgI2B44nolbfJflPODPwIsj4lbgEkqXh6My8+KpqbkkSZJGzciF3WoH4P319nXA74E3At8HdlzRJ8vMhRHxPOC99fF3BgL4KWVINEmSJDUoMr2mazKOsyupRRHRdxXG0uqrr953FcbOZptt1ncVxpKvtZVz0kknnZ6ZS1zfNYp9diVJkqQpYdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZ8/qugCRpZs2b51v/yrjXve7VdxXGzo477th3FcbSRhtt1HcVxtJJJ500Ybktu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZMx52I+IhEZER8eOlbPN/EXFbRNyt3p8TEXtExKkRcUNE3Fh/fnVEzBl67Cb1+Y+c5LlPjIic0p2SJEnSSJrxsJuZ5wInANtExIOG10fEvwKPAL6emVfU4qOATwIbAUcAhwMbAofWdZIkSdIS+urGcGi93X2CdYOywwAiYmdgF+AM4GGZuU9mvh54OHA6sEtE7DLN9ZUkSdIY6ivsfg24DNgtIu40KIyIuwA7An8EfliLX15v35yZNwy2zcwbgTfVu6+cqopFxO4RcVpEnDZVzylJkqR+9BJ2M3MBpTvC+sALOqteCvwTcHhmDvrVPhpYBJw4wVP9BFgIbDaFdTs8MzfPzM2n6jklSZLUjz5HYzgcWAC8qlO2O3Ar8LlO2TrANZl56/AT1NB8dd1GkiRJuoN5ff3izPxzRHwTeF5EPBRYl3Jh2jGZeVVn0+uB9SJitcy8rfscETEP2AD4W6d4Ub2dbN/uMhX1lyRJ0ujre5zd7oVqd7gwreMMSj2fNMHjnwTMBX7dKbu23t57eOOIWBtYYgQISZIktanvsPsj4HxgV8qFaedn5glD23y23r4nIuYPCuvP7613PzMoz8y/A+cCT4iIh3W2nwv8F6VPsCRJkmaB3roxAGRmRsSnKCEUlmzVJTOPjogdKGH4nIj4GpDAc4H7Al/KzM8PPez9lAD8s4g4FrgZ2AZYDTgL2HTq90aSJEmjpu+WXYAjKf1sbwH+e5JtdgZeC/yVckHbHpTuCnvWdXeQmZ+lDEd2GYtbjX8OPAG4biorL0mSpNHVa8tutSkldB+bmX+daIPMXETp33voROsnecxn6HRv6Nh6JeooSZKkMTQKLbtvrLcf77UWkiRJak4vLbsR8c/AdsC/AM8EvpWZp/RRF0mSJLWrr24M/wK8mzI+7rHAa3qqhyRJkhrWS9jNzCMpF6ZJkiRJ02YU+uxKkiRJ08KwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSswy7kiRJapZhV5IkSc0y7EqSJKlZhl1JkiQ1y7ArSZKkZhl2JUmS1CzDriRJkppl2JUkSVKzDLuSJElqlmFXkiRJzTLsSpIkqVmGXUmSJDXLsCtJkqRmGXYlSZLULMOuJEmSmmXYlSRJUrMMu5IkSWqWYVeSJEnNMuxKkiSpWYZdSZIkNcuwK0mSpGZFZvZdh5EVEVcBF/ddD0mSJC3TfTJzw+FCw64kSZKaZTcGSZIkNcuwK0mSpGYZdiVJktQsw64kSZKaZdiVJElSs/4/1co9LlRbN3MAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "x, y = model.predict('I did not mean to hurt you')\n",
        "test = [list(x[i].detach().numpy()) for i in range(8)]\n",
        "test = np.array(test)\n",
        "n, m, _ = np.shape(test)\n",
        "test = test.reshape(n, m)\n",
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "plt.imshow(test.T, cmap=\"gray\")\n",
        "ax.xaxis.tick_top()\n",
        "ax.xaxis.set_ticks(list(range(8)), labels = [\"je\", \"n\", \"ai\", \"pas\", \"voulu\", \"intention\", \"de\", \"blesser\"], rotation = 45)\n",
        "ax.yaxis.set_ticks(list(range(7)), labels = [\"I\", \"did\", \"not\", \"mean\", \"to\", \"hurt\", \"you\"], rotation = 0)\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "plt.savefig(\"heat_map_1.png\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
