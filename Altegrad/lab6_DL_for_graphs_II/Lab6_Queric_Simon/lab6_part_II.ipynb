{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h2>ALTeGraD 2023<br>Lab Session 6: Deep Learning for Graphs (2/2)</h2> 21 / 11 / 2023<br>Lecture: Prof. Michalis Vazirgiannis <br>\n",
    "Lab: Dr. Giannis Nikolentzos & Dr. Johannes Lutzeyer <br> <br>\n",
    "<b>Student name:</b> Simon Queric\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    Gs = list()\n",
    "    y = list()\n",
    "\n",
    "    N = range(10, 21)\n",
    "    P = [0.2, 0.4]\n",
    "    for n in N :\n",
    "        for p in P : \n",
    "            Gs.append(nx.erdos_renyi_graph(n, p))\n",
    "            y.append(1.*(p==0.2))\n",
    "    return Gs, y\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse_coo_tensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, n_class):\n",
    "        super(GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, hidden_dim_3)\n",
    "        self.fc4 = nn.Linear(hidden_dim_3, n_class)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_in, adj, idx):\n",
    "        \n",
    "        ############## Task 2\n",
    "    \n",
    "        z1 = self.relu(self.fc1(adj@x_in))\n",
    "        x = self.relu(self.fc2(adj@z1))\n",
    "        idx = idx.unsqueeze(1).repeat(1, x.size(1))\n",
    "        out= torch.zeros(int(torch.max(idx))+1, x.size(1), device=x_in.device)\n",
    "        out  = out.scatter_add_(0, idx, x) \n",
    "        out = self.fc4(self.relu(self.fc3(out)))\n",
    "\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 3.4650 acc_train: 0.5263 time: 0.0117s\n",
      "Epoch: 0011 loss_train: 0.3042 acc_train: 0.8421 time: 0.0081s\n",
      "Epoch: 0021 loss_train: 0.2739 acc_train: 0.8421 time: 0.0118s\n",
      "Epoch: 0031 loss_train: 0.2495 acc_train: 0.8421 time: 0.0085s\n",
      "Epoch: 0041 loss_train: 0.2270 acc_train: 0.8421 time: 0.0069s\n",
      "Epoch: 0051 loss_train: 0.2135 acc_train: 0.8421 time: 0.0073s\n",
      "Epoch: 0061 loss_train: 0.1877 acc_train: 0.8421 time: 0.0067s\n",
      "Epoch: 0071 loss_train: 0.1511 acc_train: 0.8947 time: 0.0119s\n",
      "Epoch: 0081 loss_train: 0.1326 acc_train: 0.8947 time: 0.0138s\n",
      "Epoch: 0091 loss_train: 0.1200 acc_train: 0.8947 time: 0.0082s\n",
      "Epoch: 0101 loss_train: 0.1068 acc_train: 0.8947 time: 0.0068s\n",
      "Epoch: 0111 loss_train: 0.1014 acc_train: 0.9474 time: 0.0068s\n",
      "Epoch: 0121 loss_train: 0.0966 acc_train: 0.9474 time: 0.0076s\n",
      "Epoch: 0131 loss_train: 0.0853 acc_train: 1.0000 time: 0.0083s\n",
      "Epoch: 0141 loss_train: 0.0850 acc_train: 1.0000 time: 0.0092s\n",
      "Epoch: 0151 loss_train: 0.0689 acc_train: 1.0000 time: 0.0080s\n",
      "Epoch: 0161 loss_train: 0.0679 acc_train: 1.0000 time: 0.0068s\n",
      "Epoch: 0171 loss_train: 0.0496 acc_train: 1.0000 time: 0.0087s\n",
      "Epoch: 0181 loss_train: 0.0355 acc_train: 1.0000 time: 0.0071s\n",
      "Epoch: 0191 loss_train: 0.3362 acc_train: 0.7895 time: 0.0073s\n",
      "Optimization finished!\n",
      "loss_test: 0.0434 acc_test: 1.0000 time: 0.0110s\n"
     ]
    }
   ],
   "source": [
    "# Initializes device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 200\n",
    "batch_size = 8\n",
    "n_hidden_1 = 16\n",
    "n_hidden_2 = 32\n",
    "n_hidden_3 = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Generates synthetic dataset\n",
    "Gs, y = create_dataset()\n",
    "n_class = np.unique(y).size\n",
    "\n",
    "# Splits the dataset into a training and a test set\n",
    "G_train, G_test, y_train, y_test = train_test_split(Gs, y, test_size=0.1)\n",
    "\n",
    "N_train = len(G_train)\n",
    "N_test = len(G_test)\n",
    "\n",
    "# Initializes model and optimizer\n",
    "model = GNN(1, n_hidden_1, n_hidden_2, n_hidden_3, n_class).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Trains the model\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for i in range(0, N_train, batch_size):\n",
    "        adj_batch = list()\n",
    "        idx_batch = list()\n",
    "        y_batch = list()\n",
    "\n",
    "        ############## Task 3\n",
    "\n",
    "        #q, r = N_train//batch_size, N_train%batch_size\n",
    "        k = min(N_train, i+batch_size)\n",
    "        y_batch = np.array(y_train[i:k])\n",
    "        y_batch = torch.from_numpy(y_batch).long()\n",
    "\n",
    "        adj_batch = [nx.adjacency_matrix(G) for G in G_train[i:k]]\n",
    "        for j in range(len(adj_batch)) :\n",
    "            idx_batch+=[j]*adj_batch[j].shape[0]\n",
    "\n",
    "        idx_batch = torch.from_numpy(np.array(idx_batch))\n",
    "        adj_batch = sp.block_diag(adj_batch)\n",
    "        adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch)\n",
    "        n = adj_batch.shape[0]\n",
    "        features_batch = torch.ones(n, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(features_batch, adj_batch, idx_batch)\n",
    "        loss = loss_function(output, y_batch)\n",
    "        train_loss += loss.item() * output.size(0)\n",
    "        count += output.size(0)\n",
    "        preds = output.max(1)[1].type_as(y_batch)\n",
    "        correct += torch.sum(preds.eq(y_batch).double())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(train_loss / count),\n",
    "              'acc_train: {:.4f}'.format(correct / count),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "        \n",
    "print('Optimization finished!')\n",
    "\n",
    "# Evaluates the model\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "count = 0\n",
    "for i in range(0, N_test, batch_size):\n",
    "    adj_batch = list()\n",
    "    idx_batch = list()\n",
    "    y_batch = list()\n",
    "\n",
    "    ############## Task 3\n",
    "    \n",
    "    k = min(N_train, i+batch_size)\n",
    "    y_batch = np.array(y_test[i:k])\n",
    "    y_batch = torch.from_numpy(y_batch).long()\n",
    "\n",
    "\n",
    "    adj_batch = [nx.adjacency_matrix(G) for G in G_test[i:k]]\n",
    "    \n",
    "    for j in range(len(adj_batch)) :\n",
    "        idx_batch += [j]*adj_batch[j].shape[0]\n",
    "    idx_batch = torch.from_numpy(np.array(idx_batch))\n",
    "\n",
    "    adj_batch = sp.block_diag(adj_batch)\n",
    "    adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch)\n",
    "    n = adj_batch.shape[0]\n",
    "\n",
    "    features_batch = torch.ones(n, 1)\n",
    "\n",
    "\n",
    "    output = model(features_batch, adj_batch, idx_batch)\n",
    "    loss = loss_function(output, y_batch)\n",
    "    test_loss += loss.item() * output.size(0)\n",
    "    count += output.size(0)\n",
    "    preds = output.max(1)[1].type_as(y_batch)\n",
    "    correct += torch.sum(preds.eq(y_batch).double())\n",
    "\n",
    "print('loss_test: {:.4f}'.format(test_loss / count),\n",
    "      'acc_test: {:.4f}'.format(correct / count),\n",
    "      'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassing(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, neighbor_aggr):\n",
    "        super(MessagePassing, self).__init__()\n",
    "        self.neighbor_aggr = neighbor_aggr\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "        self.fc2 = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \n",
    "        ############## Task 6\n",
    "    \n",
    "        x_node = self.fc1(x)\n",
    "        m = self.fc2(adj@x)\n",
    "        \n",
    "        if self.neighbor_aggr == 'sum':\n",
    "            output = x_node + m\n",
    "        elif self.neighbor_aggr == 'mean':\n",
    "            deg = torch.spmm(adj, torch.ones(x.size(0),1, device=x.device))\n",
    "            output = x_node + torch.div(m, deg)\n",
    "            \n",
    "        return output\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, neighbor_aggr, readout, dropout):\n",
    "        super(GNN, self).__init__()\n",
    "        self.readout = readout\n",
    "        self.mp1 = MessagePassing(input_dim, hidden_dim, neighbor_aggr)\n",
    "        self.mp2 = MessagePassing(hidden_dim, hidden_dim, neighbor_aggr)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, adj, idx):\n",
    "        \n",
    "        ############## Task 7\n",
    "        z1 = self.relu(self.mp1(x, adj))\n",
    "        x = self.dropout(z1)\n",
    "        x = self.relu(self.mp2(x, adj))\n",
    "        \n",
    "        if self.readout == 'sum':\n",
    "            idx = idx.unsqueeze(1).repeat(1, x.size(1))\n",
    "            out = torch.zeros(int(torch.max(idx))+1, x.size(1), device=x.device)\n",
    "            out = out.scatter_add_(0, idx, x) \n",
    "        elif self.readout == 'mean':\n",
    "            idx = idx.unsqueeze(1).repeat(1, x.size(1))\n",
    "            out = torch.zeros(torch.max(idx)+1, x.size(1), device=x.device)\n",
    "            out = out.scatter_add_(0, idx, x)\n",
    "            count = torch.zeros(torch.max(idx)+1, x.size(1), device=x.device)\n",
    "            count = count.scatter_add_(0, idx, torch.ones_like(x, device=x.device))\n",
    "            out = torch.div(out, count)\n",
    "            \n",
    "        ############## Task 7\n",
    "    \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector embeddings of the cycle graphs :\n",
      "tensor([[ 0.6056,  0.1566,  0.0486, -0.2543],\n",
      "        [ 0.6056,  0.1566,  0.0486, -0.2543],\n",
      "        [ 0.6056,  0.1566,  0.0486, -0.2543],\n",
      "        [ 0.6056,  0.1566,  0.0486, -0.2543],\n",
      "        [ 0.6056,  0.1566,  0.0486, -0.2543],\n",
      "        [ 0.6056,  0.1566,  0.0486, -0.2543],\n",
      "        [ 0.6056,  0.1566,  0.0486, -0.2543],\n",
      "        [ 0.6056,  0.1566,  0.0486, -0.2543],\n",
      "        [ 0.6056,  0.1566,  0.0486, -0.2543],\n",
      "        [ 0.6056,  0.1566,  0.0486, -0.2543]], grad_fn=<AddmmBackward0>)\n",
      "Vector embeddings of G1 and G2 :\n",
      "tensor([[ 1.5040,  0.6549, -5.3989,  1.4941],\n",
      "        [ 1.5040,  0.6549, -5.3989,  1.4941]], grad_fn=<AddmmBackward0>)\n",
      "Vector embeddings of G1 and G2 :\n",
      "tensor([[ 2.3042,  1.0128, -8.0439,  2.2944],\n",
      "        [ 2.3042,  1.0128, -8.0439,  2.2944]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_dim = 32\n",
    "output_dim = 4\n",
    "dropout = 0.0\n",
    "neighbor_aggr = 'sum'\n",
    "readout = 'mean'\n",
    "\n",
    "\n",
    "############## Task 4\n",
    "        \n",
    "Gs = [nx.cycle_graph(n) for n in range(10, 20)]\n",
    "\n",
    "\n",
    "############## Task 5\n",
    "        \n",
    "idx_batch = list()\n",
    "\n",
    "adj_batch = [nx.adjacency_matrix(G) for G in Gs]\n",
    "for j in range(len(adj_batch)) :\n",
    "    idx_batch += [j]*adj_batch[j].shape[0]\n",
    "\n",
    "idx_batch = torch.from_numpy(np.array(idx_batch))\n",
    "\n",
    "adj_batch = sp.block_diag(adj_batch)\n",
    "n = adj_batch.shape[0]\n",
    "adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch)\n",
    "\n",
    "features_batch = torch.ones(n, 1)\n",
    "\n",
    "\n",
    "############## Task 8\n",
    "        \n",
    "model = GNN(1, hidden_dim, output_dim, neighbor_aggr, readout, dropout).to(device)\n",
    "\n",
    "out = model(features_batch, adj_batch, idx_batch)\n",
    "\n",
    "print(\"Vector embeddings of the cycle graphs :\")\n",
    "print(out)\n",
    "\n",
    "############## Task 9\n",
    "        \n",
    "G1 = nx.Graph()\n",
    "G1.add_edges_from([(1, 2), (2, 3), (1, 3), (4, 5), (5, 6), (6, 4)])\n",
    "G2 = nx.cycle_graph(6)\n",
    "\n",
    "\n",
    "############## Task 10\n",
    "\n",
    "idx_batch = list()\n",
    "       \n",
    "adj_batch = [nx.adjacency_matrix(G1), nx.adjacency_matrix(G2)]\n",
    "for j in range(len(adj_batch)) :\n",
    "    idx_batch += [j]*adj_batch[j].shape[0]\n",
    "\n",
    "idx_batch = torch.from_numpy(np.array(idx_batch))\n",
    "\n",
    "adj_batch = sp.block_diag(adj_batch)\n",
    "n = adj_batch.shape[0]\n",
    "adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch)\n",
    "\n",
    "features_batch = torch.ones(n, 1)\n",
    "\n",
    "\n",
    "\n",
    "############## Task 11\n",
    "        \n",
    "hidden_dim = 32\n",
    "output_dim = 4\n",
    "dropout = 0.0\n",
    "neighbor_aggr = 'sum'\n",
    "readout = 'sum'\n",
    "\n",
    "model = GNN(1, hidden_dim, output_dim, neighbor_aggr, readout, dropout).to(device)\n",
    "\n",
    "out = model(features_batch, adj_batch, idx_batch)\n",
    "\n",
    "print(\"Vector embeddings of G1 and G2 :\")\n",
    "print(out)\n",
    "\n",
    "G1 = nx.Graph()\n",
    "G1.add_edges_from([(1, 2), (2, 3), (1, 3), (4, 5), (5, 6), (6, 4), (7, 8), (8, 9), (9, 7)])\n",
    "G2 = nx.cycle_graph(9)\n",
    "\n",
    "idx_batch = list()\n",
    "       \n",
    "adj_batch = [nx.adjacency_matrix(G1), nx.adjacency_matrix(G2)]\n",
    "for j in range(len(adj_batch)) :\n",
    "    idx_batch += [j]*adj_batch[j].shape[0]\n",
    "\n",
    "idx_batch = torch.from_numpy(np.array(idx_batch))\n",
    "\n",
    "adj_batch = sp.block_diag(adj_batch)\n",
    "n = adj_batch.shape[0]\n",
    "adj_batch = sparse_mx_to_torch_sparse_tensor(adj_batch)\n",
    "\n",
    "features_batch = torch.ones(n, 1)\n",
    "\n",
    "out = model(features_batch, adj_batch, idx_batch)\n",
    "\n",
    "print(\"Vector embeddings of G1 and G2 :\")\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
