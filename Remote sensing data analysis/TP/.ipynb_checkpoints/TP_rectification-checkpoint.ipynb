{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bffd3e8",
   "metadata": {
    "id": "4bffd3e8"
   },
   "source": [
    "# Geometric modeling of optical satellites\n",
    "\n",
    "### Name: **Simon Queric**\n",
    "\n",
    "\n",
    "In this practical session we'll learn how to deal with the geometric properties of optical images. The goals of the session are:\n",
    "\n",
    "* get familiar with the RPC camera model\n",
    "* deal with absolute and relative geolocation errors\n",
    "* be able to rectify a pair of images (prerequisite for the session on stereo matching)\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "To solve this practical session, answer the questions below. Then export the notebook with the answers using the menu option **File -> Download as -> Notebook (.ipynb)**. Then [submit the resulting file here](https://docs.google.com/forms/d/e/1FAIpQLSdP1oUowF7btj_kjDubbmOzgF1VNcCoxgRllPrdn_USHGAIPw/viewform?usp=sf_link). You will receive an automatic acknowledgement of receipt.\n",
    "\n",
    "There are **7 questions** in the notebook and corresponding code cells to fill-in with your solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340129f6",
   "metadata": {
    "id": "340129f6"
   },
   "source": [
    "## The Rational Polynomial Camera Model\n",
    "\n",
    "Image vendors, such as [Airbus Defence and Space](http://www.intelligence-airbusds.com/), [Maxar](https://www.maxar.com/) and [Planet](https://www.planet.com/), usually provide the orientation parameters of the cameras along with the images.\n",
    "To save their customers the tedious task of understanding and\n",
    "implementing a specific geometric camera model, they provide instead the *localization* and *projection* functions $L$ and $P$ associated to each image.\n",
    "\n",
    "These functions allow converting from image coordinates to coordinates\n",
    "on the globe and back. The projection function $P:\\mathbb{R}^3\\to\\mathbb{R}^2$,\n",
    "$(\\lambda, \\theta, h) \\mapsto \\textbf{x}$ returns the image coordinates, in pixels, of a given 3-space\n",
    "point represented by its spheroidal coordinates in the World Geodetic\n",
    "System (WGS 84). In that system a point of 3-space is identified by its\n",
    "longitude $\\lambda\\in[-180,180]$, latitude $\\theta\\in[-90,90]$ and\n",
    "altitude $h$, in meters, above the reference ellipsoid. The localization\n",
    "function $L:\\mathbb{R}^3\\to\\mathbb{R}^2$, $(\\textbf{x}, h) \\mapsto (\\lambda, \\theta)$ is its\n",
    "inverse with respect to the first two components. It takes a point $\\textbf{x}\n",
    "= (x, y)^\\top$ in the image domain together with an altitude $h$, and\n",
    "returns the geographic coordinates of the unique 3-space point\n",
    "$\\textbf{X} = (\\lambda, \\theta, h)$ whose altitude is $h$ and whose image is $\\textbf{x}$.\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Latitude_and_Longitude_of_the_Earth_fr.svg/500px-Latitude_and_Longitude_of_the_Earth_fr.svg.png\" alt=\"Longitude and latitude\"  width=\"400px\" />\n",
    "\n",
    "The *Rational Polynomial Coefficient* ($\\scriptsize{\\text{RPC}}$) camera model is an\n",
    "analytic description of the projection and localization functions. It\n",
    "was introduced in the late eighties by [Baltsavias and Stallmann](http://dx.doi.org/10.3929/ethz-a-004336038), and studied in\n",
    "depth by [Tao and Hu](http://eserv.asprs.org/PERS/2001journal/dec/2001_dec_1347-1357.pdf). In the $\\scriptsize{\\text{RPC}}$ model, the projection and\n",
    "localization functions are expressed as ratio of multivariate cubic\n",
    "polynomials. For example, the latitude component of the localization\n",
    "function for the image point $(x, y)$ at altitude $h$ is\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta = \\frac{\\sum_{i=1}^{20} C^{\\theta, \\tiny{\\text{NUM}}}_i \\rho_i(x, y, h)}{\\sum_{i=1}^{20} C^{\\theta, \\tiny{\\text{DEN}}}_i \\rho_i(x, y, h)}\n",
    "\\end{equation}\n",
    "\n",
    "where $C^{\\theta, \\tiny{\\text{NUM}}}_i$ (resp.\n",
    "$C^{\\theta, \\tiny{\\text{DEN}}}_i$) is the $i^{\\text{th}}$ coefficient of the\n",
    "numerator (resp. denominator) polynomial and $\\rho_{i}$ produces the\n",
    "$i^{\\text{th}}$ factor of the three variables cubic polynomial. The functions\n",
    "are defined between normalized coordinate systems.\n",
    "\n",
    "A cubic polynomial in three variables has 20 coefficients, thus each\n",
    "component of the localization and projection functions requires 40\n",
    "coefficients. With the additional 10 parameters specifying the scale and\n",
    "offset for the five variables $x, y, \\lambda, \\theta$ and $h$, the\n",
    "$\\scriptsize{\\text{RPC}}$ camera model for an image is described by a total of 170\n",
    "coefficients.\n",
    "\n",
    "<img style=\"float: right\" width=\"300px\" src=\"https://github.com/gfacciol/IS18/blob/master/fig/rpc_illustration.jpg?raw=1\" alt=\"Projection and localization functions\" />\n",
    "\n",
    "\n",
    "We will use the notations $L$ and $P$ to denote the $\\scriptsize{\\text{RPC}}$ localization and projection functions associated to the images. If needed, we may indicate the underlying image with a\n",
    "subscript: $L_u$ and $P_u$ are the $\\scriptsize{\\text{RPC}}$ functions associated to image\n",
    "$u$. Ideally, these functions should verify\n",
    "$L_u(P_u(\\lambda, \\theta, h), h) = (\\lambda, \\theta)$ and\n",
    "$P_u(L_u(x, y, h), h) = (x, y),$ but as any model the rational\n",
    "polynomial projection has a limited precision. In particular the two\n",
    "$\\scriptsize{\\text{RPC}}$ functions are not exact inverses of each other. The errors due to\n",
    "concatenating the projection and localization functions are negligible, being\n",
    "of the order of $10^{-7}$ degrees in longitude and latitude, i.e. about 1 cm\n",
    "on the ground or $\\frac{1}{100}$ of pixel in the image.\n",
    "\n",
    "Note that Pléiades images are provided by Airbus Defence and Space with\n",
    "both the localization $L$ and projection $P$ functions, while\n",
    "DigitalGlobe and Planet provide only the projection function $P$ (only\n",
    "90 coefficients). The inverse $L$ has to be estimated from $P$. This is\n",
    "can be done iteratively.\n",
    "\n",
    "\n",
    "\n",
    "## List of exercises\n",
    "\n",
    "1. Plot the projection of a 3D vertical line on the image plane by using the RPC _projection_ function: $(\\lambda, \\theta, z) \\mapsto (x, y)$ and varying $z$.\n",
    "\n",
    "2. Crop an area of interest (AOI) defined with geographic coordinates in a pair of GeoTIFF images by using their RPC functions.\n",
    "\n",
    "3. Plot the epipolar curves of a pair of images by composing the _localization_ function of the first image with the _projection_ function of the second image. Verify that epipolar curves are locally straight. Observe the relative pointing error.\n",
    "\n",
    "4. Compute the affine approximation of the RPC _projection_ functions using a first order Taylor approximation with Automatic Differentiation (AD).\n",
    "\n",
    "5. Compute the affine fundamental matrix from the affine camera matrices associated to two crops.\n",
    "\n",
    "6. Compute two epipolar resampling similarities from the affine fundamental matrix.\n",
    "\n",
    "7. Estimate the image shift that corrects the relative pointing error.\n",
    "\n",
    "8. Estimate the affine correction of the epipolar resampling similarities that best registers the ground [solution provided]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3793f1",
   "metadata": {
    "id": "817d9aaf"
   },
   "outputs": [],
   "source": [
    "# Setup code for the notebook\n",
    "\n",
    "# Execute code 'cells' like this by clicking on the 'Run'\n",
    "# button or by pressing [shift] + [Enter].\n",
    "\n",
    "# This cell only imports some python packages that will be\n",
    "# used below. It doesn't generate any output.\n",
    "\n",
    "# The following lines install the necessary packages in the colab environment\n",
    "try:\n",
    "    from google.colab import files\n",
    "\n",
    "    # download TP data and tools\n",
    "    !wget -q http://boucantrin.ovh.hw.ipol.im/static/facciolo/mvaisat/tp2.zip\n",
    "    !unzip -q -o tp2.zip\n",
    "\n",
    "    # install dependencies\n",
    "    !python -m pip -q install rpcm\n",
    "    !pip install -q 'ad @ git+https://github.com/DapengFeng/ad'\n",
    "    !python -m pip -q install numpy matplotlib scipy geojson pyproj opencv-contrib-python==4.8.0.76 rasterio srtm4 ipyleaflet\n",
    "\n",
    "except ImportError:\n",
    "    %matplotlib notebook\n",
    "    pass\n",
    "\n",
    "\n",
    "## Setup code for the notebook\n",
    "##\n",
    "# Autoreload external python modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# These are the main includes used through the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "817d9aaf",
   "metadata": {
    "id": "817d9aaf"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5674/2540543747.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mad\u001b[0m             \u001b[0;31m# automatic differentiation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrpcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msrtm4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ad'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datetime\n",
    "import pprint\n",
    "\n",
    "import ad             # automatic differentiation\n",
    "import rpcm\n",
    "import srtm4\n",
    "import rasterio       # read/write geotiffs\n",
    "import geojson\n",
    "import ipyleaflet\n",
    "import numpy as np                   # numeric linear algebra\n",
    "import matplotlib.pyplot as plt      # plotting\n",
    "\n",
    "import utils          # IO and coordinate system conversion tools\n",
    "import vistools       # display tools\n",
    "import rectification\n",
    "\n",
    "np.set_printoptions(linewidth=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef699ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ad\n",
      "  Downloading ad-1.3.2.zip (26 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[1 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m error in ad setup command: use_2to3 is invalid.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install ad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2232911f",
   "metadata": {
    "id": "2232911f",
    "tags": []
   },
   "source": [
    "# A. Dataset available for this session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce36a5e",
   "metadata": {
    "id": "0ce36a5e"
   },
   "source": [
    "Since high-resolution optical images are not freely downloadable (you have to buy them), a sample set of WorldView-3 images are provided in a remote folder. This dataset (IARPA dataset) is a collection of 49 WorldView-3 images of Buenos Aires: http://menthe.ovh.hw.ipol.im/IARPA_data\n",
    "\n",
    "The content of this folder can be listed with the `find` function of the `utils` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f3db1",
   "metadata": {
    "id": "930f3db1"
   },
   "outputs": [],
   "source": [
    "# list the tiff images available in the remote folder\n",
    "images = utils.find('http://menthe.ovh.hw.ipol.im/IARPA_data/cloud_optimized_geotif', extension='TIF')\n",
    "print('Found {} images'.format(len(images)))\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5e7b7",
   "metadata": {
    "id": "efd5e7b7"
   },
   "outputs": [],
   "source": [
    "# sort the images by acquisition date\n",
    "images.sort(key=utils.acquisition_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663c9ab",
   "metadata": {
    "id": "1663c9ab"
   },
   "source": [
    "### Images geographic footprints\n",
    "\n",
    "The longitude, latitude bounding box of an image can be read from its metadata with the `utils.lon_lat_image_footprint()` function. Let's use it to display the footprints of the first two images on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a15303",
   "metadata": {
    "id": "34a15303"
   },
   "outputs": [],
   "source": [
    "# # create a map\n",
    "m = vistools.clickablemap()\n",
    "\n",
    "# select two image indices\n",
    "i, j = 0, 1\n",
    "\n",
    "# display the footprint polygons\n",
    "for k in [i, j]:\n",
    "    footprint = utils.lon_lat_image_footprint(images[k])\n",
    "    m.add_layer(ipyleaflet.GeoJSON(data=footprint))\n",
    "\n",
    "# center the map on the center of the last footprint\n",
    "m.center = np.mean(footprint['coordinates'][0][:4], axis=0).tolist()[::-1]\n",
    "\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da9a91",
   "metadata": {
    "id": "49da9a91"
   },
   "source": [
    "Draw a (small) area of interest (AOI) within the images footprints on the map above. The coordinates of your AOI will be stored in the list `m.AOI`. The last drawn AOI is `m.AOI[-1]`\n",
    "\n",
    "If you don't draw anything, the default polygon defined in the next cell will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e07089b",
   "metadata": {
    "id": "0e07089b"
   },
   "outputs": [],
   "source": [
    "try:  # get coordinates of last AOI drawn on the map\n",
    "    aoi = m.AOIs[-1]\n",
    "\n",
    "except IndexError:  # m.AOIs is empty because no AOI was drawn\n",
    "    print(\"Using default AOI.\")\n",
    "    aoi = {'type': 'Polygon',\n",
    "           'coordinates': [[[-58.584587, -34.490861],\n",
    "                            [-58.584587, -34.489077],\n",
    "                            [-58.58301,  -34.489077],\n",
    "                            [-58.58301,  -34.490861],\n",
    "                            [-58.584587, -34.490861]]]}\n",
    "\n",
    "print(\"Selected AOI:\")\n",
    "pprint.pprint(aoi)\n",
    "\n",
    "# add center field\n",
    "aoi['center'] = np.mean(aoi['coordinates'][0][:4], axis=0).tolist()\n",
    "\n",
    "# draw the polygon and center map\n",
    "m.add_layer(ipyleaflet.GeoJSON(data=aoi))\n",
    "m.center = aoi['center'][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece5a8e9",
   "metadata": {
    "id": "ece5a8e9",
    "tags": []
   },
   "source": [
    "# B. The RPC camera model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff170f",
   "metadata": {
    "id": "e2ff170f"
   },
   "source": [
    "The 90 coefficients (20 \\* 2 \\* 2 + 10) of the RPC projection function associated to each image are stored in the image GeoTIFF header. They can be read with the `rpc_from_geotiff` function of the `rpcm` module. This function returns an instance of the class `rpcm.RPCModel` which contains the RPC coefficients and a `projection` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79abea24",
   "metadata": {
    "id": "79abea24"
   },
   "outputs": [],
   "source": [
    "rpc = rpcm.rpc_from_geotiff(images[i])\n",
    "print(type(rpc))\n",
    "pprint.pprint(vars(rpc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd200c4",
   "metadata": {
    "id": "3cd200c4"
   },
   "source": [
    "**Exercise 1.** Implement the `projection_of_3d_vertical_line` function in the cell below. This function samples the projection of a 3D vertical line on the image plane by using the RPC _projection_ function: $(\\lambda, \\theta, h) \\mapsto (x, y)$ and varying $h$.\n",
    "\n",
    "Its inputs arguments and outputs are described in its docstring.\n",
    "\n",
    "To implement this function you may use the `projection` method of the `rpcm.RPCModel` object. This method implements the RPC _projection_ function: it receives as arguments three geographic coordinates `lon, lat, h` and returns two image coordinates `x, y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005770d8",
   "metadata": {
    "id": "005770d8"
   },
   "outputs": [],
   "source": [
    "def projection_of_3d_vertical_line(rpc, lon, lat, h_min=-200, h_max=3000, h_step=10):\n",
    "    \"\"\"\n",
    "    Sample the projection of a 3D vertical line on the image plane.\n",
    "\n",
    "    Args:\n",
    "        rpc (rpcm.RPCModel): object holding the camera model\n",
    "        lon, lat (float): geographic coordinates of the ground point through\n",
    "            which the vertical line passes\n",
    "        h_min, h_max (float): min, max altitude bounds of the vertical line\n",
    "        h_step (float): step used to sample the vertical line\n",
    "\n",
    "    Return:\n",
    "        list of pairs of floats: (x, y) coordinates of points in the image\n",
    "            plane given by their pixel coordinates\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e0f092",
   "metadata": {
    "id": "20e0f092"
   },
   "source": [
    "The code below calls your `projection_of_3d_vertical_line` function to plot the image projections of the sampled vertical line passing through the center of the area that you've selected in the map. The color varies with $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade2baa",
   "metadata": {
    "id": "5ade2baa"
   },
   "outputs": [],
   "source": [
    "# plot the projection of a 3d vertical line on the image plane\n",
    "lon, lat = aoi['center']\n",
    "h_min, h_max, h_step = -100, 150, 5\n",
    "rpc = rpcm.rpc_from_geotiff(images[i])\n",
    "p = np.asarray(projection_of_3d_vertical_line(rpc, lon, lat, h_min, h_max, h_step))\n",
    "plt.figure()\n",
    "plt.scatter(p[:, 0], p[:, 1], c=np.arange(h_min, h_max, h_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cca5cd",
   "metadata": {
    "id": "b7cca5cd"
   },
   "source": [
    "**Exercise 2.** Implement the `bounding_box_of_projected_aoi` function in the cell below. This function computes the pixel coordinates of the bounding box of an AOI (area of interest) in a GeoTIFF image. The AOI is defined in geographic coordinates, and the conversion to pixel coordinates uses the image RPC functions.\n",
    "\n",
    "Its input arguments and outputs are described in the docstring.\n",
    "\n",
    "To implement this function you may use:\n",
    "* the `projection` method of the `rpcm.RPCModel` object\n",
    "* `utils.bounding_box2D` to compute a horizontal/vertical rectangular bounding box\n",
    "\n",
    "Remember that the `projection` function needs an altitude coordinate `z`, which **is not** contained in the `aoi` GeoJSON polygon. Thus the `bounding_box_of_projected_aoi` function expects an altitude argument `z`, which defaults to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed512e8",
   "metadata": {
    "id": "aed512e8"
   },
   "outputs": [],
   "source": [
    "def bounding_box_of_projected_aoi(rpc, aoi, z=0):\n",
    "    \"\"\"\n",
    "    Return the x, y, w, h pixel bounding box of a projected AOI.\n",
    "\n",
    "    Args:\n",
    "        rpc (rpcm.RPCModel): RPC camera model\n",
    "        aoi (geojson.Polygon): GeoJSON polygon representing the AOI\n",
    "        z (float, optional): altitude of the AOI with respect to the WGS84 ellipsoid\n",
    "\n",
    "    Return:\n",
    "        x, y (ints): column and row of the top-left pixel of the bounding box\n",
    "        w, h (ints): width and height of the bounding box, in pixels\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b50e4",
   "metadata": {
    "id": "3d0b50e4"
   },
   "source": [
    "The function `crop_aoi` defined below uses your `bounding_box_of_projected_aoi` function to actually crop the AOI. It uses:\n",
    "* `rpcm.rpc_from_geotiff` to read the RPC coefficients and get an `rpcm.RPCModel` object\n",
    "* the `read()` method of `rasterio` datasets to do the crop by reading the requested subset of the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd667f82",
   "metadata": {
    "id": "bd667f82"
   },
   "outputs": [],
   "source": [
    "def crop_aoi(geotiff, aoi, z=0, max_size=1e7):\n",
    "    \"\"\"\n",
    "    Crop a geographic AOI in a georeferenced image using its RPC functions.\n",
    "\n",
    "    Args:\n",
    "        geotiff (str): path or url to the input GeoTIFF image file\n",
    "        aoi (geojson.Polygon): GeoJSON polygon representing the AOI\n",
    "        z (float, optional): base altitude with respect to WGS84 ellipsoid (0\n",
    "            by default)\n",
    "        max_size (int, optional): maximum allowed size for the crop, in pixels.\n",
    "            If the area of the projected AOI bounding box exceeds this size,\n",
    "            an exception is raised.\n",
    "\n",
    "    Return:\n",
    "        crop (array): numpy array containing the cropped image\n",
    "        x, y (ints): pixel coordinates (column and row) of the top-left corner\n",
    "            of the crop in the original image\n",
    "    \"\"\"\n",
    "    x, y, w, h = bounding_box_of_projected_aoi(rpcm.rpc_from_geotiff(geotiff), aoi, z)\n",
    "    if w*h > max_size:\n",
    "        raise Exception(\"Crop too large. Please reduce your AOI.\")\n",
    "    with rasterio.open(geotiff, 'r') as src:\n",
    "        crop = src.read(window=((y, y + h), (x, x + w)), boundless=True).squeeze()\n",
    "    return crop, x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d8498",
   "metadata": {
    "id": "ca2d8498"
   },
   "source": [
    "Let's use the AOI selected in the interactive map to get an image crop and display it!\n",
    "\n",
    "The `srtm4.srtm4()` function can be used to get the altitude of the ground at coordinates `lon`, `lat`, in order to get an unbiased projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6010d70",
   "metadata": {
    "id": "e6010d70"
   },
   "outputs": [],
   "source": [
    "# get the altitude of the center of the AOI\n",
    "lon, lat  = aoi['center']\n",
    "z = srtm4.srtm4(lon, lat)\n",
    "\n",
    "# crop the selected AOI in image i\n",
    "crop, x, y = crop_aoi(images[i], aoi, z)\n",
    "\n",
    "# display the crop\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(utils.simple_equalization_8bit(crop), cmap='gray')\n",
    "\n",
    "# plot the vertical line from the previous exercise\n",
    "rpc = rpcm.rpc_from_geotiff(images[i])\n",
    "p = np.asarray(projection_of_3d_vertical_line(rpc, lon, lat, z - 100, z + 100, 5))\n",
    "plt.plot(p[:, 0] - x, p[:,  1] - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2344a",
   "metadata": {
    "id": "ced2344a"
   },
   "source": [
    "### Localization function\n",
    "\n",
    "The _localization_ function is the inverse of the _projection_ function with respect to the image coordinates. It takes as input a triplet `x, y, z`, where `x` and `y` are pixel coordinates and `z` is the altitude of the corresponding 3D point above the WGS84 ellipsoid. It returns the longitude `lon` and latitude `lat` of the 3D point.\n",
    "\n",
    "Some image providers, such as DigitalGlobe - the company that operates WorldView-3 - distribute their images with RPC coefficients for the _projection_ function only. In that cases, the _localization_ function can be evaluated by estimating iteratively the inverse of the _projection_ function. This is implemented in the `localization` method of the `rpcm.RPCModel` class.\n",
    "\n",
    "The code below picks a point in the image, localizes this image point on the ground, projects back this 3D point on the image, then **computes the distance to the original image point**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9286947",
   "metadata": {
    "id": "a9286947"
   },
   "outputs": [],
   "source": [
    "# pick the projection of the center of the AOI on the image\n",
    "lon, lat  = aoi['center']\n",
    "z = srtm4.srtm4(lon, lat)\n",
    "x, y = rpc.projection(lon, lat, z)\n",
    "\n",
    "# localize this image point on the ground\n",
    "lon, lat = rpc.localization(x, y, z)\n",
    "\n",
    "# project back this point on the image\n",
    "xx, yy = rpc.projection(lon, lat, z)\n",
    "\n",
    "# compute the distance to the original point\n",
    "print(\"Error of the inverse: {} pixels\".format(np.linalg.norm([xx - x, yy - y])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24e53a",
   "metadata": {
    "id": "ce24e53a",
    "tags": []
   },
   "source": [
    "# C. Epipolar curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dc5aff",
   "metadata": {
    "id": "66dc5aff"
   },
   "source": [
    "**Exercise 3.** Implement the `epipolar_curve` function in the cell below. This function samples an epipolar curve of a pair of images by composing the _localization_ function of the first image with the _projection_ function of the second image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537fb63",
   "metadata": {
    "id": "2537fb63"
   },
   "outputs": [],
   "source": [
    "def epipolar_curve(rpc1, rpc2, x, y, h_min=-200, h_max=3000, h_step=10):\n",
    "    \"\"\"\n",
    "    Sample the epipolar curve of image 2 associated to point (x, y) of image 1.\n",
    "\n",
    "    Args:\n",
    "        rpc1, rpc2 (rpcm.RPCModel): RPC camera models of the two images\n",
    "        x, y (float): pixel coordinates of a point in image 1\n",
    "        h_min, h_max (float): min, max altitudes defining the bounds of the epipolar curve\n",
    "        h_step (float): step used to sample the epipolar curve\n",
    "\n",
    "    Return:\n",
    "        list of points in the second image given by their pixel coordinates\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97f943",
   "metadata": {
    "id": "8b97f943"
   },
   "source": [
    "The code below calls your `epipolar_curve` function to plot the epipolar curve associated to the point `200, 200` of the first image. Observe that the epipolar curve is almost straight. Observe the relative pointing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996721b0",
   "metadata": {
    "id": "996721b0"
   },
   "outputs": [],
   "source": [
    "# select two images\n",
    "i, j = 0, 1\n",
    "\n",
    "# get the altitude of the center of the AOI\n",
    "lon, lat = aoi['center']\n",
    "z = srtm4.srtm4(lon, lat)\n",
    "\n",
    "# crop the two images\n",
    "im1, x1, y1 = crop_aoi(images[i], aoi, z)\n",
    "im2, x2, y2 = crop_aoi(images[j], aoi, z)\n",
    "\n",
    "# select a point in the first image\n",
    "x0, y0 = 208, 190\n",
    "\n",
    "# compensate the crop offset of the first image\n",
    "x, y = x0 + x1, y0 + y1\n",
    "\n",
    "# read the RPC coefficients of the two images\n",
    "rpc1 = rpcm.rpc_from_geotiff(images[i])\n",
    "rpc2 = rpcm.rpc_from_geotiff(images[j])\n",
    "\n",
    "# compute the epipolar curve\n",
    "epi = epipolar_curve(rpc1, rpc2, x, y)\n",
    "\n",
    "# compensate for the crop offset of the second image\n",
    "p = np.array([(x - x2, y - y2) for x, y in epi])\n",
    "\n",
    "# plot the epipolar curve on the second image\n",
    "f, ax = plt.subplots(1, 2, figsize=(8, 8))\n",
    "ax[0].plot(x0, y0, 'r+')\n",
    "ax[1].plot(p[:, 0], p[:, 1], 'r-', label=\"epipolar curve\")\n",
    "ax[1].plot(p[[0, -1], 0], p[[0, -1], 1], c=\"white\", ls=\"--\", alpha=1, label=\"straight line\")\n",
    "ax[1].legend()\n",
    "ax[0].imshow(np.sqrt(im1), cmap='gray')\n",
    "ax[1].imshow(np.sqrt(im2), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c682e",
   "metadata": {
    "id": "b74c682e"
   },
   "source": [
    "# D. Affine approximation of the RPC camera model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622b44e4",
   "metadata": {
    "id": "622b44e4"
   },
   "source": [
    "Let $P: \\mathbb{R}^3\\longrightarrow \\mathbb{R}^2$ be the _projection_ function. The first order Taylor approximation of $P$ around point $X_0$ is $P(X) = P(X_0) + \\nabla P(X_0)(X - X_0)$, which can be rewritten as\n",
    "\n",
    "$$\n",
    "P(X) = \\nabla P(X_0)X + T\n",
    "$$\n",
    "\n",
    "with $\\nabla P(X_0)$ the jacobian matrix of size (2, 3) and $T = P(X_0) - \\nabla P(X_0) X_0$ a vector of size 2. This can be rewritten as a linear operation by using homogeneous coordinates: with $X = (\\lambda, \\varphi, h, 1)$ the previous formula becomes $P(X) = AX$, where the (3, 4) matrix $A$ is defined by\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "\\nabla P(X_0) & T\\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$A$ is the _affine approximation_ of the RPC _projection_ function $P$ at point $X_0$. The `rpc_affine_approximation` function implemented in this exercise should compute and return $A$.\n",
    "\n",
    "The gradient $\\nabla P(X_0)$ can be computed in at least three different ways:\n",
    "* formally, as $P$ is defined by an explicit formula involving elementary operations only,\n",
    "* numerically, with a finite differences scheme as $P$ is very smooth,\n",
    "* automatically, with [Automatic Differentiation (AD)](http://pythonhosted.org/ad/), using for example\n",
    "the [jacobian](https://pythonhosted.org/ad/user_guide.html#access-to-more-than-one-derivative) function.\n",
    "\n",
    "**Exercise 4.** Implement the `rpc_affine_approximation` function. This function computes an affine approximation of the RPC _projection_ function by using a first order Taylor approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b740c",
   "metadata": {
    "id": "c33b740c"
   },
   "outputs": [],
   "source": [
    "def rpc_affine_approximation(rpc, p):\n",
    "    \"\"\"\n",
    "    Compute the first order Taylor approximation of an RPC projection function.\n",
    "\n",
    "    Args:\n",
    "        rpc (rpcm.RPCModel): RPC camera model\n",
    "        p (3-tuple of float): lon, lat, z coordinates\n",
    "\n",
    "    Return:\n",
    "        array of shape (3, 4) representing the affine camera matrix equal to the\n",
    "        first order Taylor approximation of the RPC projection function at point p.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c0a05",
   "metadata": {
    "id": "716c0a05"
   },
   "source": [
    "The code below calls your `rpc_affine_approximation` function to compute the affine camera matrix approximating the RPC _projection_ function around the center $X_0$ of the area selected in the map. Then it evaluates the approximation error away from the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da645543",
   "metadata": {
    "id": "da645543"
   },
   "outputs": [],
   "source": [
    "# get the altitude of the center of the AOI\n",
    "lon, lat = aoi['center']\n",
    "z = srtm4.srtm4(lon, lat)\n",
    "\n",
    "# compute the affine projection matrix\n",
    "A = rpc_affine_approximation(rpc, (lon, lat, z))\n",
    "\n",
    "# approximation error at the center\n",
    "err = np.linalg.norm((A @ [lon, lat, z, 1])[:2] - np.array(rpc.projection(lon, lat, z)))\n",
    "print(\"Error at the center: {} pixels\".format(err))\n",
    "\n",
    "# compute the projection in the image\n",
    "x, y = rpc.projection(lon, lat, z)\n",
    "dx, dy = 500, 500\n",
    "lon1, lat1 = rpc.localization(x + dx, y + dy, z)\n",
    "\n",
    "# approximation error at center + (dx, dy)\n",
    "err = np.linalg.norm((A @ [lon1, lat1, z, 1])[:2] - np.array(rpc.projection(lon1, lat1, z)))\n",
    "print(\"Error at {}, {} pixels from the center: {} pixels\".format(dx, dy, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e611f",
   "metadata": {
    "id": "174e611f"
   },
   "source": [
    "# E. Affine rectification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0333de",
   "metadata": {
    "id": "7d0333de"
   },
   "source": [
    "### Fundamental matrix\n",
    "\n",
    "In the affine camera model, the epipolar lines are bundles of parallel lines.  These bundles are described by a $3\\times 3$ _fundamental matrix_ $F$, such that the epipolar line of a point $\\bf x$ in the first image has the equation ${\\bf y}^T F{\\bf x}=0$ for ${\\bf y}$ in the second image.  Since all these lines are parallel, the matrix $F$ has the form $F=\\begin{bmatrix}\\begin{smallmatrix}0 & 0 & a \\\\ 0 & 0 & b \\\\ c & d & e \\end{smallmatrix}\\end{bmatrix}$.  The coefficients of $F$ are found from the two projection matrices $A$ and $B$ by imposing the homogeneous epipolarity constraints $k_1{\\bf x}=A{\\bf X}$ and $k_2{\\bf y}=B{\\bf X}$ of a generic point $\\bf X$ in space. This is a $6\\times 6$ singular system of equations whose solution can be computed explicitly\n",
    "<!-- (see e.g. Formulas 8.1. and 16.3 from Hartley-Zissserman). -->\n",
    "(see e.g. formula 17.3 (p. 412) from Hartley-Zisserman 2nd, edition).\n",
    "\n",
    "<!--\n",
    "The first formula uses the right Moore-Penrose pseudo-inverse $A^+=A^T(AA^T)^{-1}$ that has the property that $AA^+=I$ :\n",
    "\n",
    "\n",
    "$$\n",
    "F=[{\\bf e}]_\\times BA^+\n",
    "$$\n",
    "-->\n",
    "\n",
    "This formula expresses directly each entry of $F$ in terms of determinants computed from the entries of $A$ and $B$ :\n",
    "\n",
    "\n",
    "$$\n",
    "F_{ji} = (-1)^{i+j}\\mathrm{det}\\ \\begin{bmatrix}\\sim\\mathbf{a}^i \\\\ \\sim\\mathbf{b}^j\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\sim {\\bf a}^i$ denotes the matrix obtained from $A$ by omitting the row ${\\bf a}^i$. Thus, for example, the coefficient $F_{13}$ (first row, third column) is equal to the determinant of the $4\\times 4$ matrix\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}{\\bf a}^1\\\\ {\\bf a}^2 \\\\ {\\bf b }^2\\\\ {\\bf b}^3\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Exercise 5.** Implement the `affine_fundamental_matrix` function. This function computes the affine fundamental matrix from two affine camera matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f175f9ea",
   "metadata": {
    "id": "f175f9ea"
   },
   "outputs": [],
   "source": [
    "def affine_fundamental_matrix(a, b):\n",
    "    \"\"\"\n",
    "    Compute the affine fundamental matrix from two affine camera matrices.\n",
    "\n",
    "    Args:\n",
    "        a, b: arrays of shape (3, 4) representing the input camera matrices.\n",
    "\n",
    "    Return:\n",
    "        array of shape (3, 3) representing the affine fundamental matrix computed\n",
    "        with the formula 17.3 (p. 412) from Hartley & Zisserman book (2nd ed.).\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b8edd",
   "metadata": {
    "id": "127b8edd"
   },
   "source": [
    "The code below calls your `affine_fundamental_matrix` function to compute the affine fundamental matrix associated to our two images. Check that the 2, 2 top-left block of the matrix is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47933fc9",
   "metadata": {
    "id": "47933fc9"
   },
   "outputs": [],
   "source": [
    "lon, lat = aoi['center']\n",
    "z = srtm4.srtm4(lon, lat)\n",
    "\n",
    "# select two images and read their RPC coefficients\n",
    "i, j = 0, 1\n",
    "rpc1 = rpcm.rpc_from_geotiff(images[i])\n",
    "rpc2 = rpcm.rpc_from_geotiff(images[j])\n",
    "\n",
    "A = rpc_affine_approximation(rpc1, (lon, lat, z))   # affine projection matrix for first image\n",
    "B = rpc_affine_approximation(rpc2, (lon, lat, z))   # affine projection matrix for second image\n",
    "\n",
    "F = affine_fundamental_matrix(A, B)\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c02967",
   "metadata": {
    "id": "43c02967"
   },
   "source": [
    "The code below plots the epipolar lines bundles corresponding to `F`. Check that there is a one to one matching between the epipolar lines overlaid on the two images. Matching lines have the same colour, and should pass (roughly, i.e. up to a few pixels of _pointing error_) through the same scene points. Notice that the two images may be rotated with respect to each other. In that case, matching epipolar lines may be more difficult to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7094d9",
   "metadata": {
    "id": "ef7094d9"
   },
   "outputs": [],
   "source": [
    "# crop the two images\n",
    "im1, x1, y1 = crop_aoi(images[i], aoi, z)\n",
    "im2, x2, y2 = crop_aoi(images[j], aoi, z)\n",
    "\n",
    "# crops bounding boxes\n",
    "h1, w1 = im1.shape\n",
    "h2, w2 = im2.shape\n",
    "bbx1 = x1, y1, w1, h1\n",
    "bbx2 = x2, y2, w2, h2\n",
    "\n",
    "# plot epipolar lines\n",
    "svg1, svg2 = rectification.plot_epipolar_lines(F, bbx1, bbx2)\n",
    "\n",
    "vistools.display_gallery([utils.simple_equalization_8bit(im1),\n",
    "                          utils.simple_equalization_8bit(im2)],\n",
    "                         svg_overlays=[svg1, svg2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c0377",
   "metadata": {
    "id": "335c0377"
   },
   "source": [
    "### Rectifying similarities\n",
    "\n",
    "A similarity is a planar transformation that preserve shapes. It can be made of translations, rotations and scalings.\n",
    "\n",
    "If the affine fundamental matrix $F$ is given by\n",
    "$$\n",
    "F = \\begin{bmatrix}\n",
    "0 & 0 & a\\\\\n",
    "0 & 0 & b\\\\\n",
    "c & d & e\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "then two similarities $S_1$ and $S_2$ that rectify the images are given by\n",
    "\n",
    "$$\n",
    "\\texttt{S}_1 =\n",
    "\\left[\n",
    " \\begin{array}{c|c}\n",
    "  z\\texttt{R}_1 &\n",
    " \\begin{array}{c}\n",
    "     0\\\\ t\n",
    " \\end{array}\\\\\n",
    " \\hline\n",
    " \\begin{array}{cc}\n",
    "  0 & 0\n",
    "  \\end{array} & 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\qquad\\qquad\n",
    "\\texttt{S}_2 =\n",
    "\\left[\n",
    " \\begin{array}{c|c}\n",
    "  \\frac{1}{z}\\texttt{R}_2 &\n",
    " \\begin{array}{c}\n",
    "     0\\\\ -t\n",
    " \\end{array}\\\\\n",
    " \\hline\n",
    " \\begin{array}{cc}\n",
    "  0 & 0\n",
    "  \\end{array} & 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "where $z = \\sqrt{\\frac{r}{s}}$, $t = \\frac{e}{2\\sqrt{rs}}$ with $r=\\sqrt{c^2+d^2}$, $s=\\sqrt{a^2+b^2}$ and the two rotations $R_1$ and $R_2$ are given by\n",
    "\n",
    "$$\n",
    " \\texttt{R}_1 = \\frac{1}{\\sqrt{c^2 + d^2}} \\begin{bmatrix}d & -c\\\\ c & d\\end{bmatrix}\n",
    "\\qquad\\qquad\n",
    "  \\texttt{R}_2 = \\frac{1}{\\sqrt{a^2 + b^2}} \\begin{bmatrix}-b & a\\\\ -a & -b\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Exercise 6.** Implement the `rectifying_similarities` function. This function computes two similarities that transform the epipolar lines in a set of matching horizontal lines. The operation of resampling a pair of images such that the epipolar lines become horizontal and aligned is called _stereo rectification_ or _epipolar resampling_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a6db8",
   "metadata": {
    "id": "a59a6db8"
   },
   "outputs": [],
   "source": [
    "def rectifying_similarities(F, debug=False):\n",
    "    \"\"\"\n",
    "    Computes two similarities from an affine fundamental matrix.\n",
    "\n",
    "    Args:\n",
    "        F: 3x3 numpy array representing the input fundamental matrix\n",
    "        debug (optional, default is False): boolean flag to activate verbose\n",
    "            mode\n",
    "\n",
    "    Returns:\n",
    "        S, S': two similarities such that, when used to resample the two images\n",
    "            related by the fundamental matrix, the resampled images are\n",
    "            stereo-rectified.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f794b9f7",
   "metadata": {
    "id": "f794b9f7"
   },
   "source": [
    "### Putting all together\n",
    "\n",
    "The function `rectifying_affine_transforms` below uses the three functions\n",
    "\n",
    "* `rpc_affine_approximation`,\n",
    "* `affine_fundamental_matrix` and\n",
    "* `rectifying_similarities`\n",
    "\n",
    "to compute two rectifying similarities $\\texttt{S}_1$ and $\\texttt{S}_2$ for two input images on a given AOI.\n",
    "\n",
    "Then it composes them with two translations that crop the rectified images on the AOI bounding box, and computes the minimal crop size needed to ensure that the whole AOI is included in both rectified image crops.\n",
    "\n",
    "Optionally, it corrects $\\texttt{S}_2$ with a planar affine transform such that $\\texttt{S}_2\\texttt{P}_2$ and $\\texttt{S}_1\\texttt{P}_1$ both map the corners of the AOI to the same pixel coordinates, so that the rectified image crops are registered. This will be implemented in the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9a1965",
   "metadata": {
    "id": "fd9a1965"
   },
   "outputs": [],
   "source": [
    "def rectifying_affine_transforms(rpc1, rpc2, aoi, z=0, register_ground=True):\n",
    "    \"\"\"\n",
    "    Compute two affine transforms that rectify two images over a given AOI.\n",
    "\n",
    "    Args:\n",
    "        rpc1, rpc2 (rpcm.RPCModel): two RPC camera models\n",
    "        aoi (geojson.Polygon): area of interest\n",
    "\n",
    "    Return:\n",
    "        S1, S2 (2D arrays): two numpy arrays of shapes (3, 3) representing the\n",
    "            rectifying affine transforms in homogeneous coordinates\n",
    "        w, h (ints): minimal width and height of the rectified image crops\n",
    "            needed to cover the AOI in both images\n",
    "        P1, P2 (2D arrays): two numpy arrays of shapes (3, 3) representing the\n",
    "            affine camera matrices used to approximate the rpc camera models\n",
    "    \"\"\"\n",
    "    # center of the AOI\n",
    "    lons, lats = np.asarray(aoi['coordinates'][0][:4]).T\n",
    "    lon, lat = np.mean([lons, lats], axis=1)\n",
    "\n",
    "    # affine projection matrices that approximate the rpc models around the\n",
    "    # center of the AOI\n",
    "    P1 = rpc_affine_approximation(rpc1, (lon, lat, z))\n",
    "    P2 = rpc_affine_approximation(rpc2, (lon, lat, z))\n",
    "\n",
    "    # affine fundamental matrix associated to the two images\n",
    "    F = affine_fundamental_matrix(P1, P2)\n",
    "\n",
    "    # rectifying similarities\n",
    "    S1, S2 = rectifying_similarities(F)\n",
    "\n",
    "    if register_ground:\n",
    "        S1, S2 = ground_registration(aoi, z, P1, P2, S1, S2)\n",
    "\n",
    "    # aoi bounding boxes in the rectified images\n",
    "    x1, y1, w1, h1 = utils.bounding_box_of_projected_aoi(rpc1, aoi, z=z,\n",
    "                                                         homography=S1)\n",
    "    x2, y2, w2, h2 = utils.bounding_box_of_projected_aoi(rpc2, aoi, z=z,\n",
    "                                                         homography=S2)\n",
    "    S1 = utils.matrix_translation(-x1, -min(y1, y2)) @ S1\n",
    "    S2 = utils.matrix_translation(-x2, -min(y1, y2)) @ S2\n",
    "\n",
    "    w = int(round(max(w1, w2)))\n",
    "    h = int(round(max(y1 + h1, y2 + h2) - min(y1, y2)))\n",
    "    return S1, S2, w, h, P1, P2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab5914",
   "metadata": {
    "id": "18ab5914"
   },
   "source": [
    "# F.  Rectification in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd2bc0a",
   "metadata": {
    "id": "5cd2bc0a"
   },
   "source": [
    "The function `rectifying_affine_transforms` in the previous section only computes the transformations needed to rectify the images. The function `rectify_aoi` in the code cell below produces the resampled rectified image crops that are then displayed in a gallery. Flip between the images to see how the buildings move!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ab01d",
   "metadata": {
    "id": "048ab01d"
   },
   "outputs": [],
   "source": [
    "def rectify_aoi(file1, file2, aoi, z=None, correct_pointing=True,\n",
    "                register_ground=True, debug=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        file1, file2 (strings): file paths or urls of two satellite images\n",
    "        aoi (geojson.Polygon): area of interest\n",
    "        z (float, optional): base altitude with respect to WGS84 ellipsoid. If\n",
    "            None, z is retrieved from srtm.\n",
    "\n",
    "    Returns:\n",
    "        rect1, rect2: numpy arrays with the images\n",
    "        S1, S2: transformation matrices from the coordinate system of the original images\n",
    "        disp_min, disp_max: horizontal disparity range\n",
    "        P1, P2: affine rpc approximations of the two images computed during the rectification\n",
    "    \"\"\"\n",
    "    # read the RPC coefficients\n",
    "    rpc1 = rpcm.rpc_from_geotiff(file1)\n",
    "    rpc2 = rpcm.rpc_from_geotiff(file2)\n",
    "\n",
    "    # get the altitude of the center of the AOI\n",
    "    if z is None:\n",
    "        lon, lat = np.mean(aoi['coordinates'][0][:4], axis=0)\n",
    "        z = srtm4.srtm4(lon, lat)\n",
    "\n",
    "    # compute rectifying affine transforms\n",
    "    S1, S2, w, h, P1, P2 = rectifying_affine_transforms(rpc1, rpc2, aoi, z, register_ground)\n",
    "\n",
    "    # compute SIFT keypoint matches (needed to estimate the disparity range)\n",
    "    q1, q2 = rectification.sift_roi(file1, file2, aoi, z)\n",
    "\n",
    "    # correct pointing error with the SIFT keypoint matches (optional)\n",
    "    if correct_pointing:\n",
    "        S1, S2 = pointing_error_correction(S1, S2, q1, q2)\n",
    "\n",
    "    # rectify the crops\n",
    "    rect1 = rectification.affine_crop(file1, S1, w, h)\n",
    "    rect2 = rectification.affine_crop(file2, S2, w, h)\n",
    "\n",
    "    # transform the matches to the domain of the rectified images\n",
    "    q1 = utils.points_apply_homography(S1, q1)\n",
    "    q2 = utils.points_apply_homography(S2, q2)\n",
    "\n",
    "    # disparity range bounds\n",
    "    kpts_disps = (q2 - q1)[:, 0]\n",
    "    disp_min = np.percentile(kpts_disps, 5)\n",
    "    disp_max = np.percentile(kpts_disps, 100 - 5)\n",
    "\n",
    "    if debug:  # matches visualisation\n",
    "        import cv2\n",
    "        kp1 = [cv2.KeyPoint(x, y, 1, 0)  for x, y in q1]\n",
    "        kp2 = [cv2.KeyPoint(x, y, 1, 0)  for x, y in q2]\n",
    "        matches = [[cv2.DMatch(i, i, 0, 0)] for i in range(len(q1))]\n",
    "        plt.figure()\n",
    "        plt.imshow(cv2.drawMatchesKnn(utils.simple_equalization_8bit(rect1), kp1,\n",
    "                                      utils.simple_equalization_8bit(rect2), kp2,\n",
    "                                      matches, None, flags=2))\n",
    "        plt.show()\n",
    "\n",
    "    return rect1, rect2, S1, S2, disp_min, disp_max, P1, P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e3dec",
   "metadata": {
    "id": "d61e3dec"
   },
   "outputs": [],
   "source": [
    "z = srtm4.srtm4(*aoi['center'])\n",
    "rect1, rect2, S1, S2, disp_min, disp_max, P1, P2 = rectify_aoi(images[i],\n",
    "                                                               images[j],\n",
    "                                                               aoi, z=z,\n",
    "                                                               correct_pointing=False,\n",
    "                                                               register_ground=False)\n",
    "\n",
    "# display the rectified crops\n",
    "vistools.display_gallery([utils.simple_equalization_8bit(rect1),\n",
    "                          utils.simple_equalization_8bit(rect2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57ea28",
   "metadata": {
    "id": "fb57ea28"
   },
   "source": [
    "### Pointing error\n",
    "\n",
    "Inspect the two images by hovering the mouse over the numbers `0` and `1` on the left side of the image. Go back and forth to flip between the images. What do you notice?\n",
    "\n",
    "You should see only horizontal displacements, but the images are not \"vertically aligned\" (at least not perfectly): the rectification has failed! This is due to the _relative pointing error_. In the IARPA dataset, the relative pointing error is particularly visible in image pairs (0, 5) and (0, 11). In other image pairs, such as (27, 28), the error is very small and almost invisible.\n",
    "\n",
    "Using image matches, such as SIFT keypoint matches, one can estimate a vertical shift that corrects the relative pointing error. The code cell below uses the `debug` argument of the `rectify_aoi` function to show SIFT keypoint matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618d454",
   "metadata": {
    "id": "2618d454"
   },
   "outputs": [],
   "source": [
    "z = srtm4.srtm4(*aoi['center'])\n",
    "_ = rectify_aoi(images[i], images[j], aoi, z=z, correct_pointing=False, register_ground=False, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ef088",
   "metadata": {
    "id": "f83ef088"
   },
   "source": [
    "**Exercise 7.** Complete the implementation of the function `pointing_error_correction`. This function uses a list of keypoint matches to estimate the vertical parallax remaining after rectification, and corrects the rectifying transforms $\\texttt{S}_1$ and $\\texttt{S}_2$ accordingly with vertical shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57103e5c",
   "metadata": {
    "id": "57103e5c"
   },
   "outputs": [],
   "source": [
    "def pointing_error_correction(S1, S2, q1, q2):\n",
    "    \"\"\"\n",
    "    Correct rectifying similarities for the pointing error.\n",
    "\n",
    "    Args:\n",
    "        S1, S2 (np.array): two 3x3 matrices representing the rectifying similarities\n",
    "        q1, q2 (lists): two lists of matching keypoints\n",
    "\n",
    "    Returns:\n",
    "        two 3x3 matrices representing the corrected rectifying similarities\n",
    "    \"\"\"\n",
    "    # transform the matches to the domain of the rectified images\n",
    "    q1 = utils.points_apply_homography(S1, q1)\n",
    "    q2 = utils.points_apply_homography(S2, q2)\n",
    "\n",
    "    # CODE HERE: insert a few lines to correct the vertical shift\n",
    "    return S1, S2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a235d2",
   "metadata": {
    "id": "64a235d2"
   },
   "source": [
    "The next cell re-runs the `rectify_aoi` function with the option `correct_pointing=True` which calls your `pointing_error_correction` function to correct the pointing error shift. Now you should observe only horizontal displacements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d665473",
   "metadata": {
    "id": "8d665473"
   },
   "outputs": [],
   "source": [
    "z = srtm4.srtm4(*aoi['center'])\n",
    "rect1, rect2, S1, S2, disp_min, disp_max, P1, P2 = rectify_aoi(images[i],\n",
    "                                                               images[j],\n",
    "                                                               aoi, z=z,\n",
    "                                                               correct_pointing=True,\n",
    "                                                               register_ground=False)\n",
    "\n",
    "# display the rectified crops\n",
    "vistools.display_gallery([utils.simple_equalization_8bit(rect1),\n",
    "                          utils.simple_equalization_8bit(rect2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39db7b73",
   "metadata": {
    "id": "39db7b73"
   },
   "source": [
    "### Ground registration\n",
    "\n",
    "Now the rectification should be correct: there shouldn't be any vertical displacement.\n",
    "\n",
    "However, how large are the horizontal displacements? How much does the ground move? How much do the roofs of the buildings move? Knowing the camera models $\\texttt{P}_1$, $\\texttt{P}_2$ and the altitude of the ground, we should be able to compose $\\texttt{S}_2$ with a planar affine transform such that both $\\texttt{S}_2\\texttt{P}_2$ and $\\texttt{S}_1\\texttt{P}_1$ map the corners of the AOI to the same pixel coordinates, so that the rectified image crops are registered.\n",
    "\n",
    "**Exercise 8.** Implement the `ground_registration` function in the cell below. You may use the `rectification.affine_transformation` function to estimate a planar affine transformation that maps a list of 2D points onto another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af68b6",
   "metadata": {
    "id": "c0af68b6"
   },
   "outputs": [],
   "source": [
    "def ground_registration(aoi, z, P1, P2, S1, S2):\n",
    "    \"\"\"\n",
    "    Correct rectifying similarities with affine transforms to register the ground.\n",
    "\n",
    "    Args:\n",
    "        aoi (geojson.Polygon): area of interest\n",
    "        z (float): altitude of the ground\n",
    "        P1, P2 (2D arrays): two numpy arrays of shapes (3, 3) representing two\n",
    "            affine camera matrices\n",
    "        S1, S2 (2D arrays): two numpy arrays of shapes (3, 3) representing two\n",
    "            rectifying similarities in homogeneous coordinates\n",
    "    \"\"\"\n",
    "    lons, lats = np.asarray(aoi['coordinates'][0][:4]).T\n",
    "    q1 = S1 @ P1 @ [lons, lats, [z, z, z, z], [1, 1, 1, 1]]\n",
    "    q2 = S2 @ P2 @ [lons, lats, [z, z, z, z], [1, 1, 1, 1]]\n",
    "    S2 = rectification.affine_transformation(q2[:2].T, q1[:2].T) @ S2\n",
    "    return S1, S2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b1a338",
   "metadata": {
    "id": "72b1a338"
   },
   "source": [
    "The next cell re-runs the `rectify_aoi` function with the option `register_ground=True` which calls your `ground_registration` function to reduce the horizontal displacements between the two rectified images. Now you should see almost only the buildings flipping!\n",
    "\n",
    "If the ground still moves, it could be that the altitude value `z` used to estimate the registering affine transformation is not perfectly accurate. Try to increase or decrease it manually to find the best value that makes the ground perfectly registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c8280",
   "metadata": {
    "id": "d03c8280"
   },
   "outputs": [],
   "source": [
    "i, j = 38, 39\n",
    "z = srtm4.srtm4(*aoi['center']) - 12\n",
    "rect1, rect2, S1, S2, disp_min, disp_max, P1, P2 = rectify_aoi(images[i],\n",
    "                                                               images[j],\n",
    "                                                               aoi, z=z,\n",
    "                                                               correct_pointing=True,\n",
    "                                                               register_ground=True)\n",
    "\n",
    "# display the rectified crops\n",
    "vistools.display_gallery([utils.simple_equalization_8bit(rect1),\n",
    "                          utils.simple_equalization_8bit(rect2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace9ce6",
   "metadata": {
    "id": "6ace9ce6"
   },
   "source": [
    "**Corrected stereo-rectified pairs of image crops will be used as inputs for the practical session on stereo matching algorithms.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cd3042",
   "metadata": {
    "id": "72cd3042"
   },
   "source": [
    "---------------------------\n",
    "[//]: # (© 2018-2020 Carlo de Franchis, Gabriele Facciolo, Enric Meinhardt)\n",
    "[//]: # (<div style=\"text-align:center; font-size:75%;\"> Copyright © 2018-2020 Carlo de Franchis, Gabriele Facciolo, Enric Meinhardt. All rights reserved.</div>)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
